{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN34cqDbFhy417qmL1labqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/whether_a_sentence_is_about_technology_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD0MmHWomqhE"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classify sentences into two categories for simplicity.\n",
        "for example: whether a sentence is about technology or not.\n",
        "\n",
        "Dataset: We define a simple dataset class that wraps our data, using the BertTokenizer to tokenize our sentences.\n",
        "\n",
        "DataLoader: allows us to batch our data for training."
      ],
      "metadata": {
        "id": "nAkaXXCMm8Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "# Sample data: A list of tuples in the form (sentence, label)\n",
        "# Label 0: Not about technology, 1: About technology\n",
        "data = [\n",
        "    (\"I love programming with Python.\", 1),\n",
        "    (\"The weather today is beautiful.\", 0),\n",
        "    (\"Elon Musk launches another rocket.\", 1),\n",
        "    (\"Cooking is one of my favorite hobbies.\", 0)\n",
        "]\n",
        "\n",
        "# Custom dataset class\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, label = self.data[idx]\n",
        "        inputs = self.tokenizer(sentence, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),  # We squeeze the batch dimension\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load the BERT model\n",
        "# num_labels=2 parameter specifies that we have two classes.\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Create the dataset and data loader\n",
        "dataset = SimpleDataset(data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "# Example: Fine-tuning BERT (simplified for demonstration)\n",
        "# In a real-world scenario, you'd need a significantly larger dataset and more epochs to achieve good performance\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "#sets the model to training mode\n",
        "# models can have different behaviors during training and evaluation (inference) due to certain layers like dropout and batch normalization.\n",
        "model.train()\n",
        "for epoch in range(1):  # Loop over the dataset once (for demonstration)\n",
        "    for batch in dataloader:\n",
        "      #gradients would accumulate across batches. This line resets the gradients of all model parameters to zero.\n",
        "        optimizer.zero_grad()\n",
        "        # This line performs a forward pass of the model. The **batch syntax unpacks the batch dictionary into keyword arguments,\n",
        "        # effectively passing input_ids, attention_mask, and potentially other tensors to the model.\n",
        "        # The model processes this input and returns an output object.\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        # computes the gradient of the loss with respect to all model parameters (i.e., it performs backpropagation).\n",
        "        # These gradients are used by the optimizer in the next step to update the model's weights,\n",
        "        # aiming to minimize the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # For each batch, we print the loss to monitor the training process.\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# training loop for a neural network in PyTorch, encompassing setting the model to training mode, iterating over the dataset,\n",
        "# performing forward and backward passes, and computing gradients for each batch.\n",
        "# This process is aimed at fine-tuning the model's weights to improve its performance on the classification task\n",
        "#Note: This is a very simplified example. Real-world applications require more data and training epochs.\n"
      ],
      "metadata": {
        "id": "nu5jgMS5m0AE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}