{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM42jzmeN95bE3VNBc1FCuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Video_Search_and_Retrieval_System_with_Gemini%2BLLama_VLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNQjDY28M2hK"
      },
      "outputs": [],
      "source": [
        "# pip install google-generativeai transformers torch torchvision faiss-cpu opencv-python Pillow tqdm google-cloud-vision\n",
        "\n",
        "#gemini_api_key = \"your_gemini_api_key\"\n",
        "#search_engine = VideoSearchEngine(\n",
        "#    gemini_api_key=gemini_api_key,\n",
        "#    llama_model_path=\"path_to_llama_model\"\n",
        "#)\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional, NamedTuple, Union\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import google.generativeai as genai\n",
        "from google.cloud import vision\n",
        "import faiss\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class VLMFeature:\n",
        "    \"\"\"Represents features extracted by VLM models.\"\"\"\n",
        "    embedding: np.ndarray\n",
        "    description: str\n",
        "    confidence: float\n",
        "    model_source: str  # 'gemini', 'llama', or 'ensemble'\n",
        "\n",
        "@dataclass\n",
        "class VideoFrame:\n",
        "    \"\"\"Represents a single frame with VLM features.\"\"\"\n",
        "    frame_id: str\n",
        "    video_path: str\n",
        "    timestamp: float\n",
        "    features: List[VLMFeature]\n",
        "    raw_frame: Optional[np.ndarray] = None\n",
        "    width: int = 0\n",
        "    height: int = 0\n",
        "\n",
        "class GeminiVisionExtractor:\n",
        "    \"\"\"Handles feature extraction using Google's Gemini Vision model.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro-vision')\n",
        "\n",
        "    def _encode_image(self, image: Image.Image) -> str:\n",
        "        \"\"\"Convert PIL Image to base64 string.\"\"\"\n",
        "        buffered = io.BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\")\n",
        "        return base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "    async def extract_features(self, image: Image.Image) -> VLMFeature:\n",
        "        \"\"\"Extract features using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Prepare the image for Gemini\n",
        "            encoded_image = self._encode_image(image)\n",
        "\n",
        "            # Generate description using Gemini\n",
        "            prompt = \"\"\"Analyze this image in detail. Focus on:\n",
        "            1. Key objects and their relationships\n",
        "            2. Actions or processes being shown\n",
        "            3. Technical or industrial elements if present\n",
        "            4. Spatial relationships and layout\n",
        "            Provide a structured, detailed response.\"\"\"\n",
        "\n",
        "            response = await self.model.generate_content_async([prompt, encoded_image])\n",
        "            description = response.text\n",
        "\n",
        "            # Generate embedding using the description\n",
        "            # Note: Since Gemini doesn't directly provide embeddings, we'll create one from the description\n",
        "            embedding = await self.model.embed_content_async(description)\n",
        "\n",
        "            return VLMFeature(\n",
        "                embedding=np.array(embedding),\n",
        "                description=description,\n",
        "                confidence=response.candidates[0].score if hasattr(response, 'candidates') else 0.9,\n",
        "                model_source='gemini'\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Gemini feature extraction: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "class LlamaVLMExtractor:\n",
        "    \"\"\"Handles feature extraction using Llama-VLM.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"llama-vl-2\"):\n",
        "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
        "        self.model = AutoModelForVision2Seq.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    def extract_features(self, image: Image.Image) -> VLMFeature:\n",
        "        \"\"\"Extract features using Llama-VLM.\"\"\"\n",
        "        try:\n",
        "            # Prepare inputs\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "            # Generate visual features and description\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    num_beams=5,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                # Get hidden states for embedding\n",
        "                hidden_states = self.model.get_image_features(**inputs)\n",
        "                embedding = F.normalize(hidden_states.mean(dim=1), dim=-1)\n",
        "\n",
        "            # Decode the description\n",
        "            description = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "            return VLMFeature(\n",
        "                embedding=embedding.cpu().numpy()[0],\n",
        "                description=description,\n",
        "                confidence=outputs.sequences_scores[0].item() if hasattr(outputs, 'sequences_scores') else 0.9,\n",
        "                model_source='llama'\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Llama-VLM feature extraction: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "class EnsembleVLMExtractor:\n",
        "    \"\"\"Combines features from multiple VLM models.\"\"\"\n",
        "\n",
        "    def __init__(self, gemini_api_key: str, llama_model_path: str = \"llama-vl-2\"):\n",
        "        self.gemini_extractor = GeminiVisionExtractor(api_key=gemini_api_key)\n",
        "        self.llama_extractor = LlamaVLMExtractor(model_path=llama_model_path)\n",
        "\n",
        "    async def extract_features(self, image: Image.Image) -> List[VLMFeature]:\n",
        "        \"\"\"Extract features using both models and combine results.\"\"\"\n",
        "        features = []\n",
        "\n",
        "        # Get Gemini features\n",
        "        gemini_feature = await self.gemini_extractor.extract_features(image)\n",
        "        if gemini_feature:\n",
        "            features.append(gemini_feature)\n",
        "\n",
        "        # Get Llama features\n",
        "        llama_feature = self.llama_extractor.extract_features(image)\n",
        "        if llama_feature:\n",
        "            features.append(llama_feature)\n",
        "\n",
        "        # Create ensemble feature if both extractions succeeded\n",
        "        if len(features) == 2:\n",
        "            # Combine embeddings\n",
        "            combined_embedding = np.mean([f.embedding for f in features], axis=0)\n",
        "            # Combine descriptions\n",
        "            combined_description = f\"Gemini: {features[0].description}\\nLlama: {features[1].description}\"\n",
        "\n",
        "            features.append(VLMFeature(\n",
        "                embedding=combined_embedding,\n",
        "                description=combined_description,\n",
        "                confidence=np.mean([f.confidence for f in features]),\n",
        "                model_source='ensemble'\n",
        "            ))\n",
        "\n",
        "        return features\n",
        "\n",
        "class VLMVectorDatabase:\n",
        "    \"\"\"Manages vector database for VLM features.\"\"\"\n",
        "\n",
        "    def __init__(self, dimension: int = 768):  # Adjust dimension based on model output\n",
        "        self.dimension = dimension\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        self.frame_metadata: List[Tuple[VideoFrame, VLMFeature]] = []\n",
        "\n",
        "    def add_frame(self, frame: VideoFrame):\n",
        "        \"\"\"Add frame features to the index.\"\"\"\n",
        "        for feature in frame.features:\n",
        "            self.index.add(feature.embedding.reshape(1, -1))\n",
        "            self.frame_metadata.append((frame, feature))\n",
        "\n",
        "    def search(self,\n",
        "              query_embedding: np.ndarray,\n",
        "              k: int = 100,\n",
        "              min_similarity: float = 0.7) -> List[Tuple[VideoFrame, VLMFeature, float]]:\n",
        "        \"\"\"Search for similar frames using VLM features.\"\"\"\n",
        "        # Reshape query embedding\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        # Perform similarity search\n",
        "        similarities, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        # Filter and return results\n",
        "        results = []\n",
        "        for idx, similarity in zip(indices[0], similarities[0]):\n",
        "            if similarity >= min_similarity and idx < len(self.frame_metadata):\n",
        "                frame, feature = self.frame_metadata[idx]\n",
        "                results.append((frame, feature, float(similarity)))\n",
        "\n",
        "        return results\n",
        "\n",
        "class VideoSearchEngine:\n",
        "    \"\"\"Main class for video search using VLM models.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 gemini_api_key: str,\n",
        "                 llama_model_path: str = \"llama-vl-2\",\n",
        "                 frame_interval: float = 1.0):\n",
        "        self.frame_interval = frame_interval\n",
        "        self.feature_extractor = EnsembleVLMExtractor(\n",
        "            gemini_api_key=gemini_api_key,\n",
        "            llama_model_path=llama_model_path\n",
        "        )\n",
        "        self.vector_db = VLMVectorDatabase()\n",
        "\n",
        "    async def process_video(self, video_path: str):\n",
        "        \"\"\"Process and index a video file.\"\"\"\n",
        "        logger.info(f\"Processing video: {video_path}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_skip = int(fps * self.frame_interval)\n",
        "\n",
        "        frame_count = 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_skip == 0:\n",
        "                # Convert BGR to RGB and to PIL Image\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                image = Image.fromarray(frame_rgb)\n",
        "\n",
        "                # Extract features using ensemble of VLM models\n",
        "                features = await self.feature_extractor.extract_features(image)\n",
        "\n",
        "                if features:\n",
        "                    video_frame = VideoFrame(\n",
        "                        frame_id=f\"{Path(video_path).stem}_{frame_count}\",\n",
        "                        video_path=video_path,\n",
        "                        timestamp=frame_count / fps,\n",
        "                        features=features,\n",
        "                        width=frame.shape[1],\n",
        "                        height=frame.shape[0]\n",
        "                    )\n",
        "                    self.vector_db.add_frame(video_frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "    async def search_by_description(self,\n",
        "                                  query_text: str,\n",
        "                                  min_similarity: float = 0.7) -> List[Tuple[VideoFrame, VLMFeature, float]]:\n",
        "        \"\"\"Search for video segments using a text description.\"\"\"\n",
        "        # Use Gemini to generate an embedding for the query text\n",
        "        genai_embedding = await self.feature_extractor.gemini_extractor.model.embed_content_async(query_text)\n",
        "        query_embedding = np.array(genai_embedding)\n",
        "\n",
        "        # Search the database\n",
        "        return self.vector_db.search(query_embedding, min_similarity=min_similarity)\n",
        "\n",
        "    async def search_by_image(self,\n",
        "                            query_image: Image.Image,\n",
        "                            min_similarity: float = 0.7) -> List[Tuple[VideoFrame, VLMFeature, float]]:\n",
        "        \"\"\"Search for video segments using a query image.\"\"\"\n",
        "        # Extract features from query image\n",
        "        query_features = await self.feature_extractor.extract_features(query_image)\n",
        "\n",
        "        # Search using ensemble feature if available\n",
        "        ensemble_feature = next((f for f in query_features if f.model_source == 'ensemble'), None)\n",
        "        if ensemble_feature:\n",
        "            return self.vector_db.search(ensemble_feature.embedding, min_similarity=min_similarity)\n",
        "\n",
        "        # Fallback to first available feature\n",
        "        if query_features:\n",
        "            return self.vector_db.search(query_features[0].embedding, min_similarity=min_similarity)\n",
        "\n",
        "        return []\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Example usage of the VLM-based video search engine.\"\"\"\n",
        "    # Initialize the search engine with your API keys\n",
        "    search_engine = VideoSearchEngine(\n",
        "        gemini_api_key=\"your_gemini_api_key\",\n",
        "        llama_model_path=\"path_to_llama_model\"\n",
        "    )\n",
        "\n",
        "    # Process videos\n",
        "    video_dir = Path(\"path/to/videos\")\n",
        "    for video_path in video_dir.glob(\"*.mp4\"):\n",
        "        await search_engine.process_video(str(video_path))\n",
        "\n",
        "    # Example: Search by description\n",
        "    results = await search_engine.search_by_description(\n",
        "        \"Person operating industrial machinery with safety equipment\"\n",
        "    )\n",
        "\n",
        "    # Example: Search by image\n",
        "    query_image = Image.open(\"path/to/query.jpg\")\n",
        "    results = await search_engine.search_by_image(query_image)\n",
        "\n",
        "    # Print results\n",
        "    for frame, feature, similarity in results:\n",
        "        print(f\"Match in {frame.video_path} at {frame.timestamp:.2f}s\")\n",
        "        print(f\"Description: {feature.description}\")\n",
        "        print(f\"Confidence: {feature.confidence:.2%}\")\n",
        "        print(f\"Similarity: {similarity:.2%}\")\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple VLM Models:\n",
        "Gemini Vision for high-quality image understanding\n",
        "Llama-VLM for additional perspective\n",
        "Ensemble approach combining both models\n",
        "\n",
        "Enhanced Features:\n",
        "Rich text descriptions from both models\n",
        "Combined embeddings for better search accuracy\n",
        "Confidence scores from each model\n",
        "\n",
        "\n",
        "Async Processing:\n",
        "Asynchronous feature extraction\n",
        "Parallel video processing\n",
        "Efficient handling of API calls\n",
        "\n",
        "\n",
        "Flexible Search:\n",
        "Search by text description\n",
        "Search by image\n",
        "Combined feature search"
      ],
      "metadata": {
        "id": "_tAxNzvFNcWp"
      }
    }
  ]
}