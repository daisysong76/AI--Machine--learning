{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM14PDbBYr/AccG72LYNGhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Adapter_Based_Image_Compression_and_Expansion_for_Gaming_on_edge_devices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied to a real-world **adapter-based project for image compression and distributed inference in gaming**. This project will integrate adapters into a vision transformer (ViT) model and optimize deployment for inference on edge devices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Project: Adapter-Based Image Compression and Expansion for Gaming**\n",
        "\n",
        "#### **Goal**\n",
        "Use adapters to fine-tune a pre-trained Vision Transformer (ViT) for compressing and expanding game textures and optimize its deployment using distributed inference.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps**\n",
        "\n",
        "#### **1. Baseline Model**\n",
        "- Start with a pre-trained **Vision Transformer (ViT)** from Hugging Face.\n",
        "- Train the baseline model on image compression and reconstruction tasks to establish performance benchmarks.\n",
        "\n",
        "#### **2. Integrate Adapters**\n",
        "- **Design**: Use adapters for task-specific learning while keeping the core ViT model frozen.\n",
        "  - Place adapters after attention and feed-forward layers.\n",
        "  - Experiment with different bottleneck sizes (e.g., 64, 128) for the adapters.\n",
        "- **Architecture**: Each adapter has:\n",
        "  - A **down-projection** (dimensionality reduction).\n",
        "  - A **non-linear activation** (ReLU or GELU).\n",
        "  - An **up-projection** (dimensionality expansion).\n",
        "  - Residual connections to preserve information.\n",
        "\n",
        "#### **Code for Adapter Integration**"
      ],
      "metadata": {
        "id": "t7EXytr1TdVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNyBSGvKTXqj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "from transformers import ViTModel\n",
        "\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_projection = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.non_linearity = nn.ReLU()\n",
        "        self.up_projection = nn.Linear(bottleneck_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.down_projection(x)\n",
        "        x = self.non_linearity(x)\n",
        "        x = self.up_projection(x)\n",
        "        return x + residual\n",
        "\n",
        "class ViTWithAdapters(nn.Module):\n",
        "    def __init__(self, model_name=\"google/vit-base-patch16-224\", bottleneck_dim=64):\n",
        "        super(ViTWithAdapters, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained(model_name)\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add adapters to each transformer layer\n",
        "        self.adapters = nn.ModuleList(\n",
        "            [Adapter(self.vit.config.hidden_size, bottleneck_dim) for _ in range(self.vit.config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.vit(pixel_values, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states[1:]  # Skip embedding layer\n",
        "\n",
        "        adapted_hidden_states = []\n",
        "        for i, hidden_state in enumerate(hidden_states):\n",
        "            adapted_hidden_states.append(self.adapters[i](hidden_state))\n",
        "\n",
        "        return adapted_hidden_states[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-q0yh0jT00b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3. Challenges and Solutions**\n",
        "- **Bottleneck Size**: Use hyperparameter tuning frameworks like **Optuna** to find the optimal adapter dimension.\n",
        "- **Placement**: Experiment with adapter placement after attention layers, feed-forward layers, or both.\n",
        "- **Fine-Tuning Stability**: Use learning rate warm-up and gradient clipping to stabilize training.\n",
        "- **Dataset Size**: Augment the dataset with adversarial examples (e.g., compressed textures) to provide more task-specific signals.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Validation and Testing**\n",
        "- Compare the fine-tuned adapter model with the baseline ViT model.\n",
        "- Use metrics like **PSNR (Peak Signal-to-Noise Ratio)** and **SSIM (Structural Similarity Index)** for image compression quality.\n",
        "- Visualize reconstructed images to verify quality retention.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Distributed Inference Optimization**\n",
        "- For deployment, use a **hybrid edge-cloud approach**:\n",
        "  1. Compress images on the gaming device using adapters.\n",
        "  2. Offload expansion to a cloud server for high-quality rendering.\n",
        "\n",
        "- **Techniques**:\n",
        "  - **Quantization**: Reduce adapter and model weight precision (e.g., 8-bit integers).\n",
        "  - **Model Sharding**: Split the ViT model and adapters across cloud servers.\n",
        "  - **Latency Optimization**: Minimize data transfer overhead using efficient protocols (e.g., gRPC).\n",
        "\n",
        "#### **Distributed Deployment Code**"
      ],
      "metadata": {
        "id": "bopv9ekpTmLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "# Cloud server side\n",
        "def distributed_inference(model, images):\n",
        "    # Simulate model sharding by splitting layers\n",
        "    results = []\n",
        "    for image in images:\n",
        "        compressed = model.adapters[0](image)  # Compression adapter on device\n",
        "        expanded = model.adapters[-1](compressed)  # Expansion adapter on server\n",
        "        results.append(expanded)\n",
        "    return results\n",
        "\n",
        "# On-device side\n",
        "def preprocess_and_compress(image_path, model):\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "    image = transforms.ToTensor()(image_path).unsqueeze(0)\n",
        "    compressed = model.adapters[0](feature_extractor(image))\n",
        "    return compressed"
      ],
      "metadata": {
        "id": "Phfzq62FT2Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Advanced Considerations**\n",
        "1. **Dynamic Adapter Switching**:\n",
        "   - Use different adapters for low-latency vs. high-quality compression depending on the gaming scenario.\n",
        "2. **Continuous Fine-Tuning**:\n",
        "   - Implement **federated learning** to fine-tune adapters on-device using user-generated data.\n",
        "3. **Scaling Across Devices**:\n",
        "   - Use adapter ensembles for multi-tasking (e.g., combining image compression with style transfer).\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Deployment Checklist**\n",
        "- **Latency and Bandwidth**: Ensure inference meets real-time gaming requirements (<30ms per frame).\n",
        "- **Memory Footprint**: Ensure adapter sizes fit edge devices (e.g., mobile GPUs).\n",
        "- **Scalability**: Test inference across different network conditions and device specifications."
      ],
      "metadata": {
        "id": "5x2H7raGT7mi"
      }
    }
  ]
}