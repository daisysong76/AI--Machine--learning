{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH/Eys32gxKanABhyFZLBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Video_Search_and_Retrieval_System_with_spatial_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv-5-G08LgqS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional, NamedTuple\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel, DetrImageProcessor, DetrForObjectDetection\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torchvision.ops import box_iou\n",
        "import faiss\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BoundingBox(NamedTuple):\n",
        "    \"\"\"Represents a bounding box with normalized coordinates.\"\"\"\n",
        "    x1: float\n",
        "    y1: float\n",
        "    x2: float\n",
        "    y2: float\n",
        "\n",
        "    def area(self) -> float:\n",
        "        \"\"\"Calculate area of bounding box.\"\"\"\n",
        "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
        "\n",
        "    def to_pixels(self, width: int, height: int) -> Tuple[int, int, int, int]:\n",
        "        \"\"\"Convert normalized coordinates to pixel coordinates.\"\"\"\n",
        "        return (\n",
        "            int(self.x1 * width),\n",
        "            int(self.y1 * height),\n",
        "            int(self.x2 * width),\n",
        "            int(self.y2 * height)\n",
        "        )\n",
        "\n",
        "@dataclass\n",
        "class SpatialFeature:\n",
        "    \"\"\"Represents a spatial feature with its location and embedding.\"\"\"\n",
        "    embedding: np.ndarray\n",
        "    bbox: BoundingBox\n",
        "    confidence: float\n",
        "    label: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class VideoFrame:\n",
        "    \"\"\"Represents a single frame from a video with its metadata and spatial features.\"\"\"\n",
        "    frame_id: str\n",
        "    video_path: str\n",
        "    timestamp: float\n",
        "    global_embedding: np.ndarray\n",
        "    spatial_features: List[SpatialFeature]\n",
        "    frame_width: int\n",
        "    frame_height: int\n",
        "\n",
        "class SpatialFeatureExtractor:\n",
        "    \"\"\"Handles spatial feature extraction using DETR for object detection and CLIP for features.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize DETR for object detection\n",
        "        self.detr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "        self.detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "        self.detr_model.to(self.device)\n",
        "\n",
        "        # Initialize CLIP for feature extraction\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.clip_model.to(self.device)\n",
        "\n",
        "    def extract_spatial_features(self, image: Image.Image,\n",
        "                               confidence_threshold: float = 0.7) -> List[SpatialFeature]:\n",
        "        \"\"\"Extract spatial features using DETR and CLIP.\"\"\"\n",
        "\n",
        "        width, height = image.size\n",
        "\n",
        "        # Detect objects using DETR\n",
        "        with torch.no_grad():\n",
        "            inputs = self.detr_processor(images=image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            outputs = self.detr_model(**inputs)\n",
        "\n",
        "        # Process DETR outputs\n",
        "        probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "        keep = probas.max(-1).values > confidence_threshold\n",
        "\n",
        "        # Convert boxes to normalized coordinates\n",
        "        boxes = outputs.pred_boxes[0][keep]\n",
        "\n",
        "        spatial_features = []\n",
        "        for box, prob in zip(boxes, probas[keep]):\n",
        "            # Convert box to normalized coordinates\n",
        "            x1, y1, x2, y2 = box.cpu().tolist()\n",
        "            bbox = BoundingBox(\n",
        "                max(0, min(1, (x1 + 1) / 2)),\n",
        "                max(0, min(1, (y1 + 1) / 2)),\n",
        "                max(0, min(1, (x2 + 1) / 2)),\n",
        "                max(0, min(1, (y2 + 1) / 2))\n",
        "            )\n",
        "\n",
        "            # Extract region feature using CLIP\n",
        "            region = image.crop(bbox.to_pixels(width, height))\n",
        "            inputs = self.clip_processor(images=region, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            region_features = self.clip_model.get_image_features(**inputs)\n",
        "            region_features = F.normalize(region_features, dim=-1)\n",
        "\n",
        "            spatial_features.append(SpatialFeature(\n",
        "                embedding=region_features.cpu().numpy()[0],\n",
        "                bbox=bbox,\n",
        "                confidence=float(prob.max()),\n",
        "                label=self.detr_processor.id2label[prob.argmax().item()]\n",
        "            ))\n",
        "\n",
        "        return spatial_features\n",
        "\n",
        "class SpatialVectorDatabase:\n",
        "    \"\"\"Manages vector databases for both global and spatial features.\"\"\"\n",
        "\n",
        "    def __init__(self, dimension: int = 512):\n",
        "        self.dimension = dimension\n",
        "        self.global_index = faiss.IndexFlatIP(dimension)\n",
        "        self.spatial_index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        self.frame_metadata: List[VideoFrame] = []\n",
        "        self.spatial_feature_map: Dict[int, Tuple[VideoFrame, SpatialFeature]] = {}\n",
        "\n",
        "    def add_frame(self, frame: VideoFrame):\n",
        "        \"\"\"Add frame with both global and spatial features to indices.\"\"\"\n",
        "\n",
        "        self.global_index.add(frame.global_embedding.reshape(1, -1))\n",
        "\n",
        "        spatial_embeddings = []\n",
        "        for feature in frame.spatial_features:\n",
        "            spatial_embeddings.append(feature.embedding)\n",
        "\n",
        "        if spatial_embeddings:\n",
        "            spatial_embeddings = np.vstack(spatial_embeddings)\n",
        "            start_idx = self.spatial_index.ntotal\n",
        "            self.spatial_index.add(spatial_embeddings)\n",
        "\n",
        "            # Map spatial feature indices to their metadata\n",
        "            for i, feature in enumerate(frame.spatial_features):\n",
        "                self.spatial_feature_map[start_idx + i] = (frame, feature)\n",
        "\n",
        "        self.frame_metadata.append(frame)\n",
        "\n",
        "    def spatial_search(self,\n",
        "                      query_embedding: np.ndarray,\n",
        "                      query_bbox: Optional[BoundingBox] = None,\n",
        "                      k: int = 100,\n",
        "                      iou_threshold: float = 0.5) -> List[Tuple[VideoFrame, SpatialFeature, float]]:\n",
        "        \"\"\"Search for similar spatial features with optional spatial constraints.\"\"\"\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        # Perform similarity search\n",
        "        similarities, indices = self.spatial_index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for idx, similarity in zip(indices[0], similarities[0]):\n",
        "            if idx in self.spatial_feature_map:\n",
        "                frame, feature = self.spatial_feature_map[idx]\n",
        "\n",
        "                # Apply spatial constraint if query_bbox is provided\n",
        "                if query_bbox is not None:\n",
        "                    # Calculate IoU between query_bbox and feature bbox\n",
        "                    query_box = torch.tensor([[\n",
        "                        query_bbox.x1, query_bbox.y1,\n",
        "                        query_bbox.x2, query_bbox.y2\n",
        "                    ]])\n",
        "                    feature_box = torch.tensor([[\n",
        "                        feature.bbox.x1, feature.bbox.y1,\n",
        "                        feature.bbox.x2, feature.bbox.y2\n",
        "                    ]])\n",
        "                    iou = box_iou(query_box, feature_box)[0][0]\n",
        "\n",
        "                    if iou < iou_threshold:\n",
        "                        continue\n",
        "\n",
        "                results.append((frame, feature, float(similarity)))\n",
        "\n",
        "        return results\n",
        "\n",
        "class VideoProcessor:\n",
        "    \"\"\"Enhanced video processor with spatial feature extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, frame_interval: float = 1.0):\n",
        "        self.frame_interval = frame_interval\n",
        "        self.feature_extractor = FeatureExtractor(use_clip=True)\n",
        "        self.spatial_extractor = SpatialFeatureExtractor()\n",
        "\n",
        "    def extract_frames(self, video_path: str) -> List[VideoFrame]:\n",
        "        \"\"\"Extract frames with both global and spatial features.\"\"\"\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_skip = int(fps * self.frame_interval)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        frame_count = 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_skip == 0:\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                image = Image.fromarray(frame_rgb)\n",
        "\n",
        "                global_embedding = self.feature_extractor.extract_features(image)\n",
        "\n",
        "                spatial_features = self.spatial_extractor.extract_spatial_features(image)\n",
        "\n",
        "                timestamp = frame_count / fps\n",
        "                frame_id = f\"{Path(video_path).stem}_{timestamp:.2f}\"\n",
        "\n",
        "                frames.append(VideoFrame(\n",
        "                    frame_id=frame_id,\n",
        "                    video_path=video_path,\n",
        "                    timestamp=timestamp,\n",
        "                    global_embedding=global_embedding,\n",
        "                    spatial_features=spatial_features,\n",
        "                    frame_width=width,\n",
        "                    frame_height=height\n",
        "                ))\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "class VideoSearchEngine:\n",
        "    \"\"\"Enhanced search engine with spatial search capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, frame_interval: float = 1.0):\n",
        "        self.video_processor = VideoProcessor(frame_interval=frame_interval)\n",
        "        self.vector_db = SpatialVectorDatabase()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.spatial_extractor = SpatialFeatureExtractor()\n",
        "\n",
        "    def search_by_region(self,\n",
        "                        query_image: Image.Image,\n",
        "                        query_bbox: Optional[BoundingBox] = None,\n",
        "                        min_similarity: float = 0.7,\n",
        "                        iou_threshold: float = 0.5) -> List[SearchResult]:\n",
        "        \"\"\"Search for video segments using a query image and optional spatial constraints.\"\"\"\n",
        "        # Extract spatial features from query image\n",
        "        spatial_features = self.spatial_extractor.extract_spatial_features(query_image)\n",
        "\n",
        "        if not spatial_features:\n",
        "            logger.warning(\"No spatial features detected in query image\")\n",
        "            return []\n",
        "\n",
        "        # If no specific bbox is provided, use the most confident detection\n",
        "        if query_bbox is None:\n",
        "            query_feature = max(spatial_features, key=lambda x: x.confidence)\n",
        "            query_bbox = query_feature.bbox\n",
        "            query_embedding = query_feature.embedding\n",
        "        else:\n",
        "            # Extract features from the specified region\n",
        "            region = query_image.crop(query_bbox.to_pixels(*query_image.size))\n",
        "            query_embedding = self.feature_extractor.extract_features(region)\n",
        "\n",
        "        # Perform spatial search\n",
        "        similar_regions = self.vector_db.spatial_search(\n",
        "            query_embedding,\n",
        "            query_bbox,\n",
        "            iou_threshold=iou_threshold\n",
        "        )\n",
        "\n",
        "        # Filter by similarity threshold\n",
        "        similar_regions = [(frame, feature, sim)\n",
        "                          for frame, feature, sim in similar_regions\n",
        "                          if sim >= min_similarity]\n",
        "\n",
        "        # Group results by video and temporal proximity\n",
        "        return self._cluster_spatial_results(similar_regions)\n",
        "\n",
        "    def _cluster_spatial_results(self,\n",
        "                               similar_regions: List[Tuple[VideoFrame, SpatialFeature, float]],\n",
        "                               time_threshold: float = 3.0) -> List[SearchResult]:\n",
        "        \"\"\"Group similar spatial regions into video segments.\"\"\"\n",
        "        if not similar_regions:\n",
        "            return []\n",
        "\n",
        "        # Sort by video path and timestamp\n",
        "        similar_regions.sort(key=lambda x: (x[0].video_path, x[0].timestamp))\n",
        "\n",
        "        results = []\n",
        "        current_group = []\n",
        "\n",
        "        for frame, feature, similarity in similar_regions:\n",
        "            if not current_group:\n",
        "                current_group.append((frame, feature, similarity))\n",
        "                continue\n",
        "\n",
        "            prev_frame, _, _ = current_group[-1]\n",
        "\n",
        "            if (frame.video_path == prev_frame.video_path and\n",
        "                frame.timestamp - prev_frame.timestamp <= time_threshold):\n",
        "                current_group.append((frame, feature, similarity))\n",
        "            else:\n",
        "                results.append(self._create_spatial_search_result(current_group))\n",
        "                current_group = [(frame, feature, similarity)]\n",
        "\n",
        "        if current_group:\n",
        "            results.append(self._create_spatial_search_result(current_group))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _create_spatial_search_result(self,\n",
        "                                    group: List[Tuple[VideoFrame, SpatialFeature, float]]) -> SearchResult:\n",
        "        \"\"\"Create a SearchResult object from a group of spatial matches.\"\"\"\n",
        "        frames, features, similarities = zip(*group)\n",
        "        return SearchResult(\n",
        "            video_path=frames[0].video_path,\n",
        "            start_time=min(f.timestamp for f in frames),\n",
        "            end_time=max(f.timestamp for f in frames),\n",
        "            confidence=float(np.mean(similarities)),\n",
        "            matching_frames=[f.timestamp for f in frames],\n",
        "            bboxes=[f.bbox for f in features]  # Include matching regions\n",
        "        )\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the spatial video search engine.\"\"\"\n",
        "    search_engine = VideoSearchEngine(frame_interval=0.5)\n",
        "\n",
        "    # Index videos from a directory\n",
        "    search_engine.index_videos(\"path/to/videos\")\n",
        "\n",
        "    # Define a region of interest (normalized coordinates)\n",
        "    query_bbox = BoundingBox(x1=0.2, y1=0.3, x2=0.8, y2=0.9)\n",
        "\n",
        "    # Perform a spatial search using a query image and region\n",
        "    query_image = Image.open(\"path/to/query.jpg\")\n",
        "    results = search_engine.search_by_region(\n",
        "        query_image,\n",
        "        query_bbox=query_bbox,\n",
        "        min_similarity=0.7,\n",
        "        iou_threshold=0.5\n",
        "    )\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"Found match in {result.video_path}\")\n",
        "        print(f\"Time range: {result.start_time:.2f}s - {result.end_time:.2f}s\")\n",
        "        print(f\"Confidence: {result.confidence:.2%}\")\n",
        "        print(\"Matching regions:\")\n",
        "        for bbox in result.bboxes:\n",
        "            print(f\"  Region: ({bbox.x1:.2f}, {bbox.y1:.2f}) to ({bbox.x2:.2f}, {bbox.y2:.2f})\")\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}