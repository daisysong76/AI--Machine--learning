{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqYGk34g392mlLeqCZ2DXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/forecasting_future_purchases_by_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For forecasting future purchases using sequence prediction models, consider using an LSTM (Long Short-Term Memory) model due to its effectiveness in handling time-series data. After collecting and preprocessing customer transaction history, you train the LSTM model to recognize patterns in purchase sequences. Post-training, evaluate its accuracy using a separate test dataset to assess its predictive performance. Refinement involves tweaking model parameters, like the number of layers or neurons, and retraining until you achieve satisfactory forecast accuracy. This iterative process helps in accurately predicting when and what a customer might buy next, enabling timely and personalized marketing strategies."
      ],
      "metadata": {
        "id": "uGVqWFkTlZUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start by preprocessing your time-series data (customer purchase history), normalizing it, and then structuring it into sequences for the LSTM to process. Using libraries like TensorFlow or PyTorch, you define an LSTM model, train it on your data, evaluate its performance on a test set, and adjust parameters as needed for improvement"
      ],
      "metadata": {
        "id": "WxwH9M2ql0-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a project forecasting future purchases with sequence prediction models:\n"
      ],
      "metadata": {
        "id": "JwUxb8SjmyWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect Data: Gather historical customer purchase data.\n",
        "\n",
        "\n",
        "By using web scarpping"
      ],
      "metadata": {
        "id": "miauwyxNm1gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL of the website to scrape\n",
        "url = 'https://www.example.com/products'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all product elements on the page\n",
        "    products = soup.find_all('div', class_='product')\n",
        "\n",
        "    # Extract relevant information from each product\n",
        "    for product in products:\n",
        "        name = product.find('h2', class_='product-name').text.strip()\n",
        "        price = product.find('span', class_='product-price').text.strip()\n",
        "        rating = product.find('span', class_='product-rating').text.strip()\n",
        "\n",
        "        # Save the data to a CSV file or database\n",
        "        with open('products.csv', 'a') as f:\n",
        "            f.write(f'{name}, {price}, {rating}\\n')\n",
        "\n",
        "    print('Data collection complete!')\n",
        "else:\n",
        "    print('Failed to retrieve data. Status code:', response.status_code)\n"
      ],
      "metadata": {
        "id": "XwnbBu1NnDMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we're pretending to scrape product data from an imaginary website and extract information such as product name, price, and rating. The data is then saved to a CSV file for further analysis. Remember to replace 'https://www.example.com/products' with the actual URL of the website you want to scrape. Additionally, always check the website's terms of service and robots.txt file to ensure compliance with web scraping guidelines."
      ],
      "metadata": {
        "id": "YIhCyKmAqgdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess Data: Normalize and structure the data into sequences.\n"
      ],
      "metadata": {
        "id": "nkVzl-Trm3MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the collected data from the CSV file\n",
        "data = pd.read_csv('products.csv')\n",
        "\n",
        "# Normalize the price and rating columns using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "data[['price', 'rating']] = scaler.fit_transform(data[['price', 'rating']])\n",
        "\n",
        "# Structure the data into sequences\n",
        "# For example, create sequences of 5 consecutive data points\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "for i in range(len(data) - sequence_length + 1):\n",
        "    sequence = data.iloc[i:i+sequence_length]\n",
        "    sequences.append(sequence.values)\n",
        "\n",
        "# Convert the sequences to a numpy array\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "print('Data preprocessing complete!')\n"
      ],
      "metadata": {
        "id": "TygF_ATSof-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we're pretending to load the collected product data from a CSV file, normalize the 'price' and 'rating' columns using Min-Max scaling, and structure the data into sequences of consecutive data points. Each sequence contains 5 consecutive data points, but you can adjust the sequence_length variable to fit your specific needs. Finally, the sequences are converted to a numpy array for further processing."
      ],
      "metadata": {
        "id": "LgvLZnkuqkYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data: Divide data into training and testing sets.\n"
      ],
      "metadata": {
        "id": "HZbvAQcrm4xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = sequences[:, :, :-1]  # All columns except the last one\n",
        "y = sequences[:, -1, -1]  # Last column (target variable)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('Data splitting complete!')\n"
      ],
      "metadata": {
        "id": "pvjWsYbRo-KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we're pretending to split the preprocessed sequences into features (X) and target (y). The features include all columns except the last one, which represents the target variable (e.g., the next purchase). Then, we split the data into training and testing sets using the train_test_split function from scikit-learn. The testing set size is set to 20% of the total data, and we're using a random state for reproducibility."
      ],
      "metadata": {
        "id": "m9yUnDQrqoCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Model: Create an LSTM model using TensorFlow or PyTorch.\n"
      ],
      "metadata": {
        "id": "ux2BAfw9m6YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "input_size = 10  # Size of each input sequence\n",
        "hidden_size = 32  # Number of features in the hidden state of the LSTM\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "output_size = 1  # Size of the output (e.g., predicted purchase)\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "2jFamTxzpBUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this pretend example, we define an LSTM model class called LSTMModel using PyTorch's nn.Module class. The constructor initializes the LSTM and fully connected (linear) layers. The forward method defines the forward pass of the model, where the input sequence is passed through the LSTM layers and the output of the last time step is fed into a fully connected layer to produce the final output. Finally, we create an instance of this model with the desired input, hidden, and output sizes."
      ],
      "metadata": {
        "id": "DlEAzPagqrVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model: Train the LSTM on the training set.\n"
      ],
      "metadata": {
        "id": "y_ABH9CSm8Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
        "        inputs = inputs.float()  # Convert inputs to float if necessary\n",
        "        labels = labels.float()  # Convert labels to float if necessary\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "print('Training complete!')\n"
      ],
      "metadata": {
        "id": "Rj24dYzwpiZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "We define hyperparameters such as learning rate and number of epochs.\n",
        "We define a loss function (mean squared error) and an optimizer (Adam).\n",
        "We iterate over each epoch and within each epoch, iterate over each batch in the training DataLoader.\n",
        "We perform a forward pass, compute the loss, perform a backward pass, and update the model parameters.\n",
        "We print the average loss for each epoch.\n",
        "Make sure to replace train_loader with your actual DataLoader containing the training data. Additionally, ensure that your input data (inputs) and labels (labels) are appropriately formatted and converted to PyTorch tensors."
      ],
      "metadata": {
        "id": "PO-myMLFpn99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate: Assess the model's performance on the test set."
      ],
      "metadata": {
        "id": "C57gU7x8pvyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a variable to accumulate the total loss\n",
        "total_loss = 0.0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:  # Assuming test_loader is your DataLoader\n",
        "        inputs = inputs.float()  # Convert inputs to float if necessary\n",
        "        labels = labels.float()  # Convert labels to float if necessary\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Accumulate the total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "# Calculate the average loss\n",
        "average_loss = total_loss / len(test_loader)\n",
        "\n",
        "# Print the average loss\n",
        "print(f'Average Loss on Test Set: {average_loss}')\n"
      ],
      "metadata": {
        "id": "MqzYnrV3pwYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "We set the model to evaluation mode using model.eval().\n",
        "We define a variable total_loss to accumulate the total loss.\n",
        "We iterate over each batch in the test DataLoader, performing a forward pass to compute the predicted outputs and calculating the loss between the predicted outputs and the actual labels.\n",
        "We accumulate the total loss across all batches.\n",
        "Finally, we calculate the average loss by dividing the total loss by the number of batches in the test DataLoader and print the result.\n",
        "Make sure to replace test_loader with your actual DataLoader containing the test data. Additionally, ensure that your input data (inputs) and labels (labels) are appropriately formatted and converted to PyTorch tensors."
      ],
      "metadata": {
        "id": "EYksFSUTp4Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDIbDTPRphps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refine: Adjust model parameters and retrain for improved accuracy.\n"
      ],
      "metadata": {
        "id": "N-S8lQIEm_c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model architecture with adjusted parameters\n",
        "refined_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(refined_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the refined model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    refined_model.train()\n",
        "\n",
        "    # Initialize total loss for this epoch\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over the training dataset\n",
        "    for inputs, labels in train_loader:  # Assuming train_loader is your DataLoader\n",
        "        inputs = inputs.float()  # Convert inputs to float if necessary\n",
        "        labels = labels.float()  # Convert labels to float if necessary\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = refined_model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss}')\n",
        "\n",
        "# Evaluate the refined model\n",
        "# (Code for evaluation is the same as provided in the previous response)\n",
        "\n",
        "# Optionally, save the refined model\n",
        "torch.save(refined_model.state_dict(), 'refined_model.pth')\n"
      ],
      "metadata": {
        "id": "sa5ddTzMp7Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "We define a new instance of the model (refined_model) with adjusted parameters. You might adjust parameters such as the number of layers, hidden size, or learning rate based on insights gained from previous training.\n",
        "We define a new loss function and optimizer for the refined model.\n",
        "We train the refined model similarly to before, iterating over the training dataset for a specified number of epochs, performing forward and backward passes, and updating the model parameters.\n",
        "After training, we can evaluate the refined model using the same evaluation procedure as before.\n",
        "Optionally, you can save the refined model's state dictionary for future use. Adjust the file path as needed.\n",
        "Ensure that you adjust the parameters (input_size, hidden_size, num_layers, output_size, learning_rate, num_epochs) and DataLoader (train_loader) according to your specific requirements and dataset."
      ],
      "metadata": {
        "id": "_pYALsTwp7-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy: Use the model to predict future customer purchases.\n",
        "Load the trained model: Load the saved model weights or instantiate a new instance of the model with the same architecture and load the weights into it.\n",
        "\n",
        "Prepare input data: Prepare the input data for prediction. This could involve selecting relevant features from your dataset, normalizing the data if necessary, and structuring it into sequences similar to how you did during training.\n",
        "\n",
        "Perform inference: Use the trained model to perform inference on the input data. Pass the input data through the model and obtain the model's predictions.\n",
        "\n",
        "Post-processing (optional): Perform any necessary post-processing on the model predictions. This could involve converting the predictions into a human-readable format, mapping them back to original categories or labels, or applying any business logic.\n",
        "\n",
        "Use predictions: Use the model predictions to make decisions or take actions in your application or business process. For example, you could use the predictions to personalize marketing campaigns, recommend products to users, or optimize inventory management."
      ],
      "metadata": {
        "id": "8Uqms3annBkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Load the trained model\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "model.load_state_dict(torch.load('path_to_saved_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# 2. Prepare input data (replace this with your actual input data preparation code)\n",
        "input_data = prepare_input_data_for_prediction()\n",
        "\n",
        "# 3. Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_data)\n",
        "\n",
        "# 4. Post-processing (optional)\n",
        "# Perform any necessary post-processing on the model predictions\n",
        "\n",
        "# 5. Use predictions\n",
        "# Use the model predictions for decision-making or further processing\n"
      ],
      "metadata": {
        "id": "UdpNIE9RqHUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "Replace 'path_to_saved_model.pth' with the path to your saved model file.\n",
        "Replace prepare_input_data_for_prediction() with your actual data preparation code to prepare input data for prediction.\n",
        "The model.eval() call is used to set the model to evaluation mode before performing inference.\n",
        "The torch.no_grad() context manager is used to disable gradient calculation during inference to reduce memory usage and speed up computation.\n",
        "Make sure to adjust the code according to your specific use case and requirements."
      ],
      "metadata": {
        "id": "YhmHQIDWqTs4"
      }
    }
  ]
}