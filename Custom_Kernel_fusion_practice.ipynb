{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Custom_Kernel_fusion_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# Define the CUDA kernel as a string\n",
        "cuda_kernel = \"\"\"\n",
        "extern \"C\" __global__ void custom_sin_kernel(float *input, float *output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = sinf(input[idx]);\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Define the C++ wrapper\n",
        "cpp_wrapper = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "void custom_sin_cuda(torch::Tensor input, torch::Tensor output) {\n",
        "    int size = input.numel();\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (size + blockSize - 1) / blockSize;\n",
        "\n",
        "    custom_sin_kernel<<<gridSize, blockSize>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &custom_sin_cuda, \"Custom sin CUDA kernel\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the custom extension\n",
        "custom_sin_module = load_inline(\n",
        "    name=\"custom_sin_module\",\n",
        "    cpp_sources=[cpp_wrapper],\n",
        "    cuda_sources=[cuda_kernel],\n",
        "    functions=['forward'],\n",
        "    extra_cuda_cflags=['-lcudart'],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Usage in PyTorch\n",
        "class CustomSin(nn.Module):\n",
        "    def forward(self, x):\n",
        "        output = torch.empty_like(x)\n",
        "        custom_sin_module.forward(x, output)\n",
        "        return output\n",
        "\n",
        "# Example Usage\n",
        "input_tensor = torch.randn(1024, device='cuda')\n",
        "custom_sin_layer = CustomSin().cuda()\n",
        "output_tensor = custom_sin_layer(input_tensor)\n",
        "\n",
        "# Verify the result\n",
        "reference_output = torch.sin(input_tensor)\n",
        "print(torch.allclose(output_tensor, reference_output)) # Should print True"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "k4P-JIy9jy2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of a Real-World Custom Kernel Example:**\n",
        "\n",
        "1.  **CUDA Kernel (`cuda_kernel`):**\n",
        "    * This string defines the actual CUDA kernel.\n",
        "    * `__global__` indicates that this function runs on the GPU.\n",
        "    * `blockIdx.x`, `blockDim.x`, and `threadIdx.x` are used to calculate the global thread index.\n",
        "    * `sinf()` is the single-precision sine function in CUDA's math library.\n",
        "    * The kernel calculates the sine of each element in the input tensor and stores it in the output tensor.\n",
        "\n",
        "2.  **C++ Wrapper (`cpp_wrapper`):**\n",
        "    * This C++ code acts as an interface between PyTorch and the CUDA kernel.\n",
        "    * `torch/extension.h` provides PyTorch's C++ API.\n",
        "    * `cuda_runtime.h` provides CUDA runtime functions.\n",
        "    * `custom_sin_cuda` function:\n",
        "        * Calculates the grid and block dimensions for the CUDA kernel launch.\n",
        "        * Launches the CUDA kernel using `<<<gridSize, blockSize>>>`.\n",
        "        * Obtains the raw data pointers from the Pytorch Tensors using `input.data_ptr<float>()`\n",
        "    * `PYBIND11_MODULE`: Creates a Python module that exposes the `custom_sin_cuda` function to PyTorch.\n",
        "\n",
        "3.  **`load_inline`:**\n",
        "    * This PyTorch function compiles the CUDA kernel and C++ wrapper into a PyTorch extension.\n",
        "    * `cuda_sources` and `cpp_sources` provide the source code.\n",
        "    * `extra_cuda_cflags` provides extra flags to the cuda compiler.\n",
        "    * `verbose=True` prints the compilation output.\n",
        "\n",
        "4.  **`CustomSin` Module:**\n",
        "    * This PyTorch `nn.Module` wraps the custom CUDA kernel.\n",
        "    * The `forward` method calls the `custom_sin_module.forward` function, which in turn launches the CUDA kernel.\n",
        "\n",
        "5.  **Usage:**\n",
        "    * The example creates an input tensor on the GPU.\n",
        "    * It creates an instance of the `CustomSin` module.\n",
        "    * It calls the module's `forward` method to execute the custom kernel.\n",
        "    * It then checks if the custom kernel produced the same output as the Pytorch function `torch.sin()`.\n",
        "\n",
        "**Real-World Use Cases:**\n",
        "\n",
        "* **Optimized Activation Functions:** Implement custom activation functions that are faster than standard PyTorch functions.\n",
        "* **Specialized Mathematical Operations:** Implement highly optimized mathematical operations that are not available in PyTorch.\n",
        "* **Image Processing:** Implement custom image processing kernels for operations like convolution, filtering, or color space conversion.\n",
        "* **Signal Processing:** Implement custom signal processing kernels for operations like FFT, filtering, or modulation.\n",
        "* **Graph Neural Networks:** Implement custom graph convolution kernels for specialized graph structures.\n",
        "* **Any Computationally Intensive Operation:** If you have a computationally intensive operation that is a bottleneck in your model, you can implement it as a custom CUDA kernel to improve performance."
      ],
      "metadata": {
        "id": "z19REr5pjy22"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}