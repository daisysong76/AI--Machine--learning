{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwbawsOreGOFUheDfY23Wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Amazon_Personalized_Product_Recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0EmpK_52S-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are two specific **use cases** for adapters at companies like **Amazon** and **Apple**, focusing on scenarios where this approach could shine.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Amazon: Personalized Product Recommendations**\n",
        "**Use Case**: Fine-tuning pre-trained large language models (LLMs) for personalized product recommendations across millions of users.\n",
        "\n",
        "#### **Scenario**:\n",
        "Amazon uses a foundational transformer model trained on large-scale text data, such as customer reviews, product descriptions, and behavioral data. To cater to diverse customer preferences, adapters can be added to the pre-trained model for personalization tasks.\n",
        "\n",
        "#### **Adapter Workflow**:\n",
        "1. **Goal**: Predict product recommendations based on user interaction history.\n",
        "2. **Process**:\n",
        "   - Insert adapters after feed-forward and attention layers of the transformer model.\n",
        "   - Fine-tune the adapters on specific customer groups (e.g., regional preferences, seasonal demands) while freezing the core model.\n",
        "3. **Example**:\n",
        "   - Train adapters for users in different regions (e.g., North America, Europe) to capture region-specific preferences.\n",
        "   - Train another set of adapters for holiday shopping preferences like \"Black Friday\" vs. \"Prime Day.\"\n",
        "4. **Advantages**:\n",
        "   - Reduce the computational cost by not fine-tuning the entire model for every customer segment.\n",
        "   - Quickly deploy personalized adapters for new use cases (e.g., a new product line).\n",
        "5. **Challenges**:\n",
        "   - Handling sparse datasets for niche customer segments.\n",
        "   - Ensuring inference latency remains low during high-traffic periods.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Apple: Voice Recognition and Personalization in Siri**\n",
        "**Use Case**: Enhancing Siri's voice recognition and response accuracy for specific user accents or languages.\n",
        "\n",
        "#### **Scenario**:\n",
        "Apple uses foundational models for automatic speech recognition (ASR) and natural language understanding (NLU) in Siri. Instead of retraining the entire model to support a new accent or dialect, adapters can be added for quick adaptation.\n",
        "\n",
        "#### **Adapter Workflow**:\n",
        "1. **Goal**: Fine-tune Siri's model to improve recognition for a specific accent (e.g., Australian English or Indian English).\n",
        "2. **Process**:\n",
        "   - Freeze the core ASR/NLU model trained on a generic English corpus.\n",
        "   - Add adapters trained specifically on datasets from Australian or Indian speakers.\n",
        "   - Deploy the adapted model to users in these regions without requiring significant changes to the core architecture.\n",
        "3. **Example**:\n",
        "   - Use adapters to specialize in domain-specific tasks like medical terminology or legal language.\n",
        "   - Train adapters for new device-specific commands (e.g., \"AirPods Pro gestures\" or \"Apple Vision Pro navigation\").\n",
        "4. **Advantages**:\n",
        "   - Improves Siri's accuracy for specific user demographics without retraining the full model.\n",
        "   - Enables efficient deployment to edge devices like iPhones and Apple Watches with minimal storage overhead.\n",
        "5. **Challenges**:\n",
        "   - Ensuring adapters don't degrade performance for general users.\n",
        "   - Maintaining real-time processing speeds on devices with limited computational power.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Use Cases**\n",
        "\n",
        "| **Aspect**             | **Amazon: Recommendations**                                     | **Apple: Siri Personalization**                            |\n",
        "|-------------------------|-----------------------------------------------------------------|-----------------------------------------------------------|\n",
        "| **Model Type**          | Transformer-based LLM for text understanding                   | Transformer-based ASR and NLU models                      |\n",
        "| **Task**                | Product recommendations based on user interaction              | Voice recognition and response accuracy for accents       |\n",
        "| **Dataset**             | Customer reviews, clickstream data, purchase history           | Speech samples, language corpora, accent-specific datasets |\n",
        "| **Adapter Placement**   | After attention layers to capture user-specific preferences    | After feed-forward layers to capture accent-specific nuances |\n",
        "| **Deployment**          | Server-side for personalized recommendations                  | On-device for real-time voice interactions                |\n",
        "| **Challenges**          | Data sparsity, latency during peak traffic                     | Balancing accuracy and latency on edge devices            |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Benefits of Adapters in These Use Cases**\n",
        "1. **Efficiency**: Adapters add minimal parameters, reducing the cost and time of fine-tuning large models.\n",
        "2. **Scalability**: Different adapters can be trained and deployed for various tasks or regions, allowing rapid scaling.\n",
        "3. **Compatibility**: Adapters can work seamlessly with existing deployment pipelines (e.g., server-side inference or on-device processing).\n",
        "\n",
        "These examples demonstrate how companies like Amazon and Apple could use adapters to enhance their ML workflows, optimize resource usage, and improve user experience."
      ],
      "metadata": {
        "id": "nJF70xb1525K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Amazon: Personalized Product Recommendations\n",
        "use a pre-trained transformer model (e.g., GPT, BERT) as the backbone and leverage adapters for personalization. The approach includes dynamic adapter switching, multi-level personalization, and optimization for scalability and low-latency inference."
      ],
      "metadata": {
        "id": "U1ERwAEu3yby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Adapter Modules:\n",
        "\n",
        "Adapters are dynamically selected based on user profiles (e.g., region, preferences, season).\n",
        "Incorporate a gating mechanism to automatically route requests to the appropriate adapter."
      ],
      "metadata": {
        "id": "tD3fRDgQ4B13"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-oMucmQ3yJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Task Learning:\n",
        "\n",
        "Use shared adapters for global preferences and specialized adapters for region-specific, seasonal, or user-segment-specific preferences."
      ],
      "metadata": {
        "id": "xdD9SG2R4FcM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jajNJkXY3w2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Federated Adapter Fine-Tuning:\n",
        "\n",
        "Train adapters on-device using anonymized data to improve personalization while ensuring privacy."
      ],
      "metadata": {
        "id": "49t3Q83P4Jzm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYojAfyl4NFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized Distributed Inference:\n",
        "\n",
        "Employ model sharding and low-latency communication for scalable inference during high-traffic periods."
      ],
      "metadata": {
        "id": "p7d1cGyf4NfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n"
      ],
      "metadata": {
        "id": "TGZqd0Dw5d7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Foundation Model Setup\n",
        "Use a pre-trained LLM (e.g., GPT or BERT) trained on Amazonâ€™s large-scale text data, including customer reviews, product descriptions, and clickstream data."
      ],
      "metadata": {
        "id": "O0YZ4KuA4TOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-trained LLM as the Backbone\n",
        "\n",
        "Model Choice: Use a pre-trained model like GPT-2, BERT, or DistilBERT from the Hugging Face library. These models have already learned a rich set of language representations, which we can fine-tune for our personalized recommendation use case.\n",
        "\n",
        "Data Types:\n",
        "Customer reviews: Capture user sentiment about products.\n",
        "Product descriptions: Provide context for recommendations.\n",
        "Clickstream data: Understand user preferences and behavior."
      ],
      "metadata": {
        "id": "1GpLY53B6wCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Synthetic Data\n",
        "1. Customer Reviews\n",
        "\n",
        "Schema:\n",
        "review_id: Unique ID for the review.\n",
        "product_id: ID of the product being reviewed.\n",
        "user_id: ID of the user leaving the review.\n",
        "review_text: Text of the review.\n",
        "star_rating: User rating (1 to 5).\n",
        "timestamp: When the review was created.\n"
      ],
      "metadata": {
        "id": "oynUD6KJ7OgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Customer Reviews=[\n",
        "  {\n",
        "    \"review_id\": \"R001\",\n",
        "    \"product_id\": \"P101\",\n",
        "    \"user_id\": \"U001\",\n",
        "    \"review_text\": \"This laptop has excellent battery life and is very lightweight.\",\n",
        "    \"star_rating\": 5,\n",
        "    \"timestamp\": \"2024-01-10T15:30:00Z\"\n",
        "  },\n",
        "  {\n",
        "    \"review_id\": \"R002\",\n",
        "    \"product_id\": \"P102\",\n",
        "    \"user_id\": \"U002\",\n",
        "    \"review_text\": \"The headphones are decent but not very comfortable for long use.\",\n",
        "    \"star_rating\": 3,\n",
        "    \"timestamp\": \"2024-01-09T18:45:00Z\"\n",
        "  },\n",
        "  {\n",
        "    \"review_id\": \"R003\",\n",
        "    \"product_id\": \"P103\",\n",
        "    \"user_id\": \"U003\",\n",
        "    \"review_text\": \"Great camera quality but overpriced for the features offered.\",\n",
        "    \"star_rating\": 4,\n",
        "    \"timestamp\": \"2024-01-08T11:20:00Z\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "ndO7oPHh4WH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product Descriptions\n",
        "\n",
        "Schema:\n",
        "\n",
        "product_id: ID of the product.\n",
        "\n",
        "product_name: Name of the product.\n",
        "\n",
        "product_description: Text describing the product.\n",
        "\n",
        "category: Product category."
      ],
      "metadata": {
        "id": "EQhl6a4E7clW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Product Descriptions=[\n",
        "  {\n",
        "    \"product_id\": \"P101\",\n",
        "    \"product_name\": \"Ultrabook X300\",\n",
        "    \"product_description\": \"A sleek and lightweight laptop with a 15-hour battery life, perfect for professionals on the go.\",\n",
        "    \"category\": \"Electronics\"\n",
        "  },\n",
        "  {\n",
        "    \"product_id\": \"P102\",\n",
        "    \"product_name\": \"Noise-Cancelling Headphones Pro\",\n",
        "    \"product_description\": \"High-quality headphones with active noise cancellation, great sound clarity, and a comfortable fit.\",\n",
        "    \"category\": \"Audio\"\n",
        "  },\n",
        "  {\n",
        "    \"product_id\": \"P103\",\n",
        "    \"product_name\": \"4K DSLR Camera\",\n",
        "    \"product_description\": \"A professional-grade DSLR camera with 4K video recording and advanced low-light performance.\",\n",
        "    \"category\": \"Photography\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "0omXX_tF7j4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clickstream Data\n",
        "\n",
        "Schema: user_id: ID of the user.\n",
        "\n",
        "session_id: Unique session identifier.\n",
        "\n",
        "timestamp: When the action occurred.\n",
        "\n",
        "event_type: Type of user action (view, click, add_to_cart, purchase).\n",
        "\n",
        "product_id: ID of the product involved in the action."
      ],
      "metadata": {
        "id": "OlbZgh5L7oV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Clickstream=[\n",
        "  {\n",
        "    \"user_id\": \"U001\",\n",
        "    \"session_id\": \"S001\",\n",
        "    \"timestamp\": \"2024-01-10T14:00:00Z\",\n",
        "    \"event_type\": \"view\",\n",
        "    \"product_id\": \"P101\"\n",
        "  },\n",
        "  {\n",
        "    \"user_id\": \"U001\",\n",
        "    \"session_id\": \"S001\",\n",
        "    \"timestamp\": \"2024-01-10T14:15:00Z\",\n",
        "    \"event_type\": \"add_to_cart\",\n",
        "    \"product_id\": \"P101\"\n",
        "  },\n",
        "  {\n",
        "    \"user_id\": \"U002\",\n",
        "    \"session_id\": \"S002\",\n",
        "    \"timestamp\": \"2024-01-09T18:00:00Z\",\n",
        "    \"event_type\": \"view\",\n",
        "    \"product_id\": \"P102\"\n",
        "  },\n",
        "  {\n",
        "    \"user_id\": \"U002\",\n",
        "    \"session_id\": \"S002\",\n",
        "    \"timestamp\": \"2024-01-09T18:10:00Z\",\n",
        "    \"event_type\": \"purchase\",\n",
        "    \"product_id\": \"P102\"\n",
        "  },\n",
        "  {\n",
        "    \"user_id\": \"U003\",\n",
        "    \"session_id\": \"S003\",\n",
        "    \"timestamp\": \"2024-01-08T10:45:00Z\",\n",
        "    \"event_type\": \"view\",\n",
        "    \"product_id\": \"P103\"\n",
        "  },\n",
        "  {\n",
        "    \"user_id\": \"U003\",\n",
        "    \"session_id\": \"S003\",\n",
        "    \"timestamp\": \"2024-01-08T11:00:00Z\",\n",
        "    \"event_type\": \"click\",\n",
        "    \"product_id\": \"P103\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "NGklsB7b7vUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Integration\n",
        "\n",
        "Merge Datasets:\n",
        "\n",
        "Combine customer reviews, product descriptions, and clickstream data using product_id and user_id as keys.\n",
        "Example: A user viewed a product (clickstream), purchased it, and left a review.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Convert review_text and product_description into embeddings using a pre-trained LLM (e.g., BERT or GPT).\n",
        "Encode user behavior (view, add_to_cart, purchase) as numerical features for model input.\n",
        "\n",
        "Train Model:\n",
        "\n",
        "Train adapters to predict product relevance or purchase likelihood using the combined dataset."
      ],
      "metadata": {
        "id": "lZ8r-Rvm75sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load customer reviews\n",
        "customer_reviews = pd.DataFrame([\n",
        "    {\"review_id\": \"R001\", \"product_id\": \"P101\", \"user_id\": \"U001\", \"review_text\": \"This laptop has excellent battery life and is very lightweight.\", \"star_rating\": 5, \"timestamp\": \"2024-01-10T15:30:00Z\"},\n",
        "    {\"review_id\": \"R002\", \"product_id\": \"P102\", \"user_id\": \"U002\", \"review_text\": \"The headphones are decent but not very comfortable for long use.\", \"star_rating\": 3, \"timestamp\": \"2024-01-09T18:45:00Z\"},\n",
        "    {\"review_id\": \"R003\", \"product_id\": \"P103\", \"user_id\": \"U003\", \"review_text\": \"Great camera quality but overpriced for the features offered.\", \"star_rating\": 4, \"timestamp\": \"2024-01-08T11:20:00Z\"}\n",
        "])\n",
        "\n",
        "# Load product descriptions\n",
        "product_descriptions = pd.DataFrame([\n",
        "    {\"product_id\": \"P101\", \"product_name\": \"Ultrabook X300\", \"product_description\": \"A sleek and lightweight laptop with a 15-hour battery life, perfect for professionals on the go.\", \"category\": \"Electronics\"},\n",
        "    {\"product_id\": \"P102\", \"product_name\": \"Noise-Cancelling Headphones Pro\", \"product_description\": \"High-quality headphones with active noise cancellation, great sound clarity, and a comfortable fit.\", \"category\": \"Audio\"},\n",
        "    {\"product_id\": \"P103\", \"product_name\": \"4K DSLR Camera\", \"product_description\": \"A professional-grade DSLR camera with 4K video recording and advanced low-light performance.\", \"category\": \"Photography\"}\n",
        "])\n",
        "\n",
        "# Load clickstream data\n",
        "clickstream_data = pd.DataFrame([\n",
        "    {\"user_id\": \"U001\", \"session_id\": \"S001\", \"timestamp\": \"2024-01-10T14:00:00Z\", \"event_type\": \"view\", \"product_id\": \"P101\"},\n",
        "    {\"user_id\": \"U001\", \"session_id\": \"S001\", \"timestamp\": \"2024-01-10T14:15:00Z\", \"event_type\": \"add_to_cart\", \"product_id\": \"P101\"},\n",
        "    {\"user_id\": \"U002\", \"session_id\": \"S002\", \"timestamp\": \"2024-01-09T18:00:00Z\", \"event_type\": \"view\", \"product_id\": \"P102\"},\n",
        "    {\"user_id\": \"U002\", \"session_id\": \"S002\", \"timestamp\": \"2024-01-09T18:10:00Z\", \"event_type\": \"purchase\", \"product_id\": \"P102\"},\n",
        "    {\"user_id\": \"U003\", \"session_id\": \"S003\", \"timestamp\": \"2024-01-08T10:45:00Z\", \"event_type\": \"view\", \"product_id\": \"P103\"},\n",
        "    {\"user_id\": \"U003\", \"session_id\": \"S003\", \"timestamp\": \"2024-01-08T11:00:00Z\", \"event_type\": \"click\", \"product_id\": \"P103\"}\n",
        "])\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = pd.merge(customer_reviews, product_descriptions, on=\"product_id\")\n",
        "merged_data = pd.merge(merged_data, clickstream_data, on=[\"user_id\", \"product_id\"], how=\"left\")\n",
        "print(merged_data.head())\n"
      ],
      "metadata": {
        "id": "z_oBXzj88JfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hT8eMUVK8I3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Adapter Integration\n",
        "Add adapters after attention and feed-forward layers in the transformer.\n",
        "Use a bottleneck dimension (e.g., 64) to balance performance and efficiency."
      ],
      "metadata": {
        "id": "Jk53XnSv4Z2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_projection = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.non_linearity = nn.ReLU()\n",
        "        self.up_projection = nn.Linear(bottleneck_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.down_projection(x)\n",
        "        x = self.non_linearity(x)\n",
        "        x = self.up_projection(x)\n",
        "        return x + residual"
      ],
      "metadata": {
        "id": "HVJ7lQiB5llV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_projection = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.non_linearity = nn.ReLU()\n",
        "        self.up_projection = nn.Linear(bottleneck_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.down_projection(x)\n",
        "        x = self.non_linearity(x)\n",
        "        x = self.up_projection(x)\n",
        "        return x + residual\n",
        "\n",
        "# class TransformerWithAdapters(nn.Module):\n",
        "#     def __init__(self, transformer_model, bottleneck_dim=64, num_adapters=5):\n",
        "#         super(TransformerWithAdapters, self).__init__()\n",
        "#         self.transformer = transformer_model\n",
        "#         self.adapters = nn.ModuleDict({\n",
        "#             f\"adapter_{i}\": Adapter(self.transformer.config.hidden_size, bottleneck_dim)\n",
        "#             for i in range(num_adapters)\n",
        "#         })\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask, adapter_key):\n",
        "#         outputs = self.transformer(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "#         hidden_states = outputs.hidden_states\n",
        "\n",
        "#         # Apply selected adapter\n",
        "#         adapted_states = self.adapters[adapter_key](hidden_states[-1])  # Use last hidden state\n",
        "#         return adapted_states\n"
      ],
      "metadata": {
        "id": "wuDc06wH4ftO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerWithAdapters(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", bottleneck_dim=64, num_adapters=5):\n",
        "        super(TransformerWithAdapters, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False  # Freeze the backbone\n",
        "\n",
        "        # Add adapters for each user segment\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            f\"adapter_{i}\": Adapter(self.bert.config.hidden_size, bottleneck_dim)\n",
        "            for i in range(num_adapters)\n",
        "        })\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)  # Predict relevance score\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, adapter_key):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states[-1]  # Use the last hidden state\n",
        "\n",
        "        # Pass through the selected adapter\n",
        "        adapted_states = self.adapters[adapter_key](hidden_states[:, 0, :])  # Use CLS token\n",
        "        logits = self.classifier(adapted_states)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "6B1f2IJl5rlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3: Personalization Workflow\n",
        "A. User-Specific Adapters\n",
        "Train separate adapters for each user segment (e.g., regional preferences, seasonal preferences):\n",
        "Dataset: Split data by region (e.g., North America, Europe) and seasonal demand (e.g., \"Black Friday\" vs. \"Prime Day\").\n",
        "Fine-Tuning: Use only the adapter parameters for training while freezing the core model.\n",
        "B. Dynamic Adapter Selection\n",
        "Use a gating mechanism based on metadata (e.g., user location, browsing history) to select the appropriate adapter during inference."
      ],
      "metadata": {
        "id": "7xsLJ6SC4jqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdapterSelector(nn.Module):\n",
        "    def __init__(self, input_dim, num_adapters):\n",
        "        super(AdapterSelector, self).__init__()\n",
        "        self.gate = nn.Linear(input_dim, num_adapters) # 128: embedding dimension for metadata\n",
        "\n",
        "    def forward(self, user_metadata):\n",
        "        adapter_weights = self.gate(user_metadata)\n",
        "        adapter_key = torch.argmax(adapter_weights, dim=1)  # Select the most relevant adapter\n",
        "        return adapter_key\n"
      ],
      "metadata": {
        "id": "w_Q-7noA53q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "dataset = load_dataset(\"amazon_us_reviews\", \"Digital_Software_v1_00\", split=\"train[:5000]\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"review_body\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Preprocess dataset\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"star_rating\"])\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(encoded_dataset))\n",
        "test_size = len(encoded_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(encoded_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "9iHN47MV50uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4: Federated Adapter Fine-Tuning\n",
        "Use federated learning to fine-tune adapters on-device, preserving user privacy:\n",
        "Fine-tune adapters locally with anonymized user data.\n",
        "Periodically aggregate adapter updates using secure aggregation."
      ],
      "metadata": {
        "id": "SPnfUjl24piE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerWithAdapters(num_adapters=5).to(device)\n",
        "adapter_selector = AdapterSelector(input_dim=16, num_adapters=5).to(device)  # Dummy metadata dim\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(adapter_selector.parameters()), lr=1e-4\n",
        ")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training\n",
        "for epoch in range(3):  # Train for 3 epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        ratings = batch[\"star_rating\"].float().unsqueeze(1).to(device)\n",
        "\n",
        "        # Simulate user metadata (random for this example)\n",
        "        user_metadata = torch.rand(input_ids.size(0), 16).to(device)\n",
        "        adapter_keys = adapter_selector(user_metadata)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = torch.cat(\n",
        "            [model(input_ids[i:i+1], attention_mask[i:i+1], f\"adapter_{adapter_keys[i].item()}\") for i in range(input_ids.size(0))]\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "RvbzJiPU6GMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluation and Validation\n",
        "Metrics:\n",
        "Accuracy: Precision and recall for personalized recommendations.\n",
        "Coverage: Fraction of users covered by adapters.\n",
        "Latency: Time to infer recommendations during high-traffic periods.\n",
        "Testing:\n",
        "Compare the performance of the adapter-enhanced model to:\n",
        "The baseline model without adapters.\n",
        "Full model fine-tuning approaches."
      ],
      "metadata": {
        "id": "-IZgAxXx4uAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Loop\n",
        "model.eval()\n",
        "adapter_selector.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        ratings = batch[\"star_rating\"].float().unsqueeze(1).to(device)\n",
        "\n",
        "        # Simulate user metadata (random for this example)\n",
        "        user_metadata = torch.rand(input_ids.size(0), 16).to(device)\n",
        "        adapter_keys = adapter_selector(user_metadata)\n",
        "\n",
        "        outputs = torch.cat(\n",
        "            [model(input_ids[i:i+1], attention_mask[i:i+1], f\"adapter_{adapter_keys[i].item()}\") for i in range(input_ids.size(0))]\n",
        "        )\n",
        "\n",
        "        # Evaluate predictions\n",
        "        correct += ((outputs.round() == ratings.round()).sum().item())\n",
        "        total += ratings.size(0)\n",
        "\n",
        "print(f\"Accuracy: {correct / total:.2f}\")\n"
      ],
      "metadata": {
        "id": "ls-ZR_qp6NBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6: Optimized Distributed Inference\n",
        "A. Model Sharding:\n",
        "Divide the model and adapters across multiple servers:\n",
        "Store global adapters on central servers.\n",
        "Cache region-specific adapters on edge servers.\n",
        "B. Quantization:\n",
        "Apply 8-bit quantization to reduce memory usage and improve inference speed for adapters.\n",
        "C. Edge-Cloud Hybrid Deployment:\n",
        "Perform initial recommendation generation on edge devices.\n",
        "Use cloud servers for complex operations (e.g., reranking results)."
      ],
      "metadata": {
        "id": "spNgEVDl4yUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenges and Solutions\n",
        "Challenge 1: Sparse Data for Niche Segments\n",
        "Solution: Use multi-task learning to share adapters between related segments, allowing them to learn from overlapping data.\n",
        "\n",
        "Challenge 2: Low-Latency Inference\n",
        "Solution:\n",
        "Use adapter caching for frequently accessed segments.\n",
        "Implement approximate nearest neighbor (ANN) search for fast user-to-adapter mapping.\n",
        "\n",
        "Challenge 3: Scalability\n",
        "Solution:\n",
        "Use adapter distillation to reduce the number of active adapters during inference.\n",
        "Dynamically load adapters based on traffic patterns."
      ],
      "metadata": {
        "id": "9M7f6ush49jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment Example\n",
        "Training:\n",
        "\n",
        "Train adapters for regions (e.g., US, EU) and special events (e.g., \"Holiday Season\").\n",
        "Use federated learning to refine adapters locally.\n",
        "Inference:\n",
        "\n",
        "Input: User metadata (e.g., region, preferences).\n",
        "Select adapter: Use the gating mechanism to choose the appropriate adapter.\n",
        "Generate recommendations: Pass user data through the selected adapter and transformer."
      ],
      "metadata": {
        "id": "dlEPgbR45GBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(model, input_data, user_metadata):\n",
        "    adapter_key = adapter_selector(user_metadata)\n",
        "    recommendations = model(input_data[\"input_ids\"], input_data[\"attention_mask\"], adapter_key)\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "Vb0Xi1Fi5JCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of This Approach\n",
        "Cost-Efficient Personalization:\n",
        "Only adapter parameters are fine-tuned, reducing computational overhead.\n",
        "\n",
        "Scalable Deployment:\n",
        "Adapters can scale across millions of users with dynamic selection and caching.\n",
        "\n",
        "Privacy-Preserving:\n",
        "Federated learning ensures that user data remains local."
      ],
      "metadata": {
        "id": "xhO2URiI5OMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features of This project\n",
        "\n",
        "Adapters:\n",
        "Dynamically switch based on user metadata.\n",
        "Train separately for different user segments.\n",
        "\n",
        "Scalability:\n",
        "Efficient use of adapters reduces computational overhead.\n",
        "\n",
        "Dynamic Selection:\n",
        "AdapterSelector routes data to the correct adapter based on metadata.\n",
        "\n",
        "Personalization:\n",
        "Adapters specialize in user-specific preferences (e.g., regions, seasonal events)."
      ],
      "metadata": {
        "id": "dXByCHVI6XNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Parallel Adapters\n",
        "\n",
        "Key Idea: Instead of inserting adapters sequentially into the transformer layers, parallel adapters are attached alongside the main layer computations (e.g., attention or feed-forward layers).\n",
        "\n",
        "Advantages:\n",
        "Reduces interference with the original model's flow.\n",
        "Improves flexibility by learning parallel paths for task-specific adaptation.\n",
        "Applications: Multitask learning where tasks are vastly different."
      ],
      "metadata": {
        "id": "FKVYXbUY9fj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParallelAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super(ParallelAdapter, self).__init__()\n",
        "        self.down_projection = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.non_linearity = nn.ReLU()\n",
        "        self.up_projection = nn.Linear(bottleneck_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        adapted = self.up_projection(self.non_linearity(self.down_projection(x)))\n",
        "        return x + adapted  # Combine with original flow in parallel\n"
      ],
      "metadata": {
        "id": "xFxtO8ER9nWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. HyperAdapters\n",
        "\n",
        "Key Idea: Use a hypernetwork to generate adapter parameters dynamically based on task metadata (e.g., task embeddings, user preferences, or contextual features).\n",
        "\n",
        "Advantages:\n",
        "Allows adaptation to a wide range of tasks without explicitly training separate adapters.\n",
        "Scales efficiently for multitask and few-shot learning.\n",
        "Applications: Personalized assistants, multilingual models, and dynamic task adaptation."
      ],
      "metadata": {
        "id": "bs8yzfGX9rFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64, task_embedding_dim=32):\n",
        "        super(HyperAdapter, self).__init__()\n",
        "        self.task_embedding_to_adapter = nn.Linear(task_embedding_dim, input_dim * bottleneck_dim + bottleneck_dim * input_dim)\n",
        "\n",
        "    def forward(self, x, task_embedding):\n",
        "        # Generate adapter weights dynamically\n",
        "        adapter_weights = self.task_embedding_to_adapter(task_embedding)\n",
        "        down_projection_weights = adapter_weights[:input_dim * bottleneck_dim].reshape(input_dim, bottleneck_dim)\n",
        "        up_projection_weights = adapter_weights[input_dim * bottleneck_dim:].reshape(bottleneck_dim, input_dim)\n",
        "\n",
        "        # Apply adapter transformation\n",
        "        down_projected = x @ down_projection_weights\n",
        "        up_projected = down_projected @ up_projection_weights\n",
        "        return x + up_projected\n"
      ],
      "metadata": {
        "id": "-mRUVatO9t7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA (Low-Rank Adaptation of Large Language Models)\n",
        "\n",
        "Key Idea: Decompose the adapter weights into two low-rank matrices to further reduce the number of parameters.\n",
        "\n",
        "Advantages:\n",
        "Highly efficient for fine-tuning massive models like GPT or T5.\n",
        "Significantly reduces memory requirements.\n",
        "Applications: Large-scale language models for domain-specific tasks."
      ],
      "metadata": {
        "id": "FWaMJJcD92CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, rank=4):\n",
        "        super(LoRAAdapter, self).__init__()\n",
        "        self.low_rank_A = nn.Linear(input_dim, rank, bias=False)\n",
        "        self.low_rank_B = nn.Linear(rank, input_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.low_rank_B(self.low_rank_A(x))  # Add low-rank transformation\n"
      ],
      "metadata": {
        "id": "l1MnII8r-D1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Task-Attention Adapters\n",
        "\n",
        "Key Idea: Use attention mechanisms within adapters to focus on task-relevant information dynamically.\n",
        "\n",
        "Advantages:\n",
        "Captures nuanced task-specific relationships.\n",
        "Allows the adapter to adaptively decide which parts of the input to emphasize.\n",
        "Applications: Multi-task and multi-domain models."
      ],
      "metadata": {
        "id": "5PCTMlBk-HyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskAttentionAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64):\n",
        "        super(TaskAttentionAdapter, self).__init__()\n",
        "        self.query = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.key = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.value = nn.Linear(input_dim, bottleneck_dim)\n",
        "        self.output_layer = nn.Linear(bottleneck_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        attention_weights = torch.softmax(Q @ K.transpose(-1, -2), dim=-1)\n",
        "        attended = attention_weights @ V\n",
        "        return x + self.output_layer(attended)  # Add residual connection\n"
      ],
      "metadata": {
        "id": "cD6azxWV-Ke6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Mixture-of-Adapters (MoA)\n",
        "\n",
        "Key Idea: Use a gating mechanism to dynamically route inputs to different adapters, enabling specialization for sub-tasks.\n",
        "\n",
        "Advantages:\n",
        "Allows the model to scale efficiently for multiple tasks.\n",
        "Improves performance by specializing adapters for sub-groups.\n",
        "Applications: Multilingual tasks or fine-grained user personalization."
      ],
      "metadata": {
        "id": "We_Ao0TC-QwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MixtureOfAdapters(nn.Module):\n",
        "    def __init__(self, input_dim, bottleneck_dim=64, num_adapters=4):\n",
        "        super(MixtureOfAdapters, self).__init__()\n",
        "        self.adapters = nn.ModuleList([Adapter(input_dim, bottleneck_dim) for _ in range(num_adapters)])\n",
        "        self.gate = nn.Linear(input_dim, num_adapters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        adapter_weights = torch.softmax(self.gate(x), dim=-1)  # Compute gating scores\n",
        "        outputs = torch.stack([adapter(x) * weight for adapter, weight in zip(self.adapters, adapter_weights.T)], dim=0)\n",
        "        return torch.sum(outputs, dim=0)  # Weighted sum of adapters\n"
      ],
      "metadata": {
        "id": "dlEeLQm4-S5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Prompt-Tuning with Adapters\n",
        "\n",
        "Key Idea: Combine prompt-tuning (embedding-based task conditioning) with adapter modules to achieve lightweight adaptation.\n",
        "\n",
        "Advantages:\n",
        "Efficient for text generation or natural language understanding tasks.\n",
        "Maintains high generalization with minimal overhead.\n",
        "Applications: Few-shot learning and prompt engineering for LLMs."
      ],
      "metadata": {
        "id": "O98fR0uv-ah4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Comparison of Methods**\n",
        "\n",
        "| **Method**                 | **Parameters** | **Speed** | **Specialization** | **Best For**                                       |\n",
        "|----------------------------|----------------|-----------|---------------------|---------------------------------------------------|\n",
        "| **Parallel Adapters**       | Moderate       | Fast      | High               | Multitask learning                                |\n",
        "| **HyperAdapters**           | Low            | Fast      | Very High          | Personalized or dynamic tasks                    |\n",
        "| **LoRA**                    | Very Low       | Very Fast | High               | Fine-tuning large-scale LLMs                     |\n",
        "| **Task-Attention Adapters** | Moderate       | Moderate  | Very High          | Multitask learning with nuanced relationships    |\n",
        "| **Mixture-of-Adapters**     | High           | Moderate  | High               | Multilingual tasks or diverse sub-groups         |\n",
        "| **Prompt-Tuning Adapters**  | Very Low       | Very Fast | Moderate           | Few-shot learning or lightweight text tasks      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Right Adapter Method**\n",
        "\n",
        "1. **For Efficiency**: Use **LoRA** or **Prompt-Tuning** for large-scale models.\n",
        "2. **For Flexibility**: Use **HyperAdapters** or **Task-Attention Adapters** for dynamic or multi-task scenarios.\n",
        "3. **For Scalability**: Use **Mixture-of-Adapters** for multilingual or multi-domain tasks.\n",
        "\n",
        "These cutting-edge adapter methods are highly flexible and efficient, offering solutions for a wide range of real-world applications."
      ],
      "metadata": {
        "id": "rySGn2Jq-gLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, LoRA stands out as the most popular adapter method currently, owing to its efficiency and integration into widely used machine learning frameworks. Other methods like HyperAdapters and Task-Attention Adapters are emerging but have not yet reached the same level of adoption."
      ],
      "metadata": {
        "id": "QrVoYpzX_Gt3"
      }
    }
  ]
}