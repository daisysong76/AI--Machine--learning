{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCAV6T2cQ6Rwjc50/bRMlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Model_Fine_tuning_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing the weights within various layers through fine-tuning for specific tasks can enhance the performance of layers showing low accuracy. This approach can be achieved by training with more task-specific datasets, allowing the model to better adapt to specific representation reading demands.\n",
        "\n",
        "we fine-tune a pre-trained language model (like BERT) for a sentiment analysis task using a task-specific dataset. This example will demonstrate how to adjust the model to improve its performance on this specific task, which can be especially beneficial for layers that initially show low accuracy in understanding sentiment.\n",
        "\n",
        "Prerequisites:\n",
        "A pre-trained model (e.g., BERT from the Hugging Face Transformers library)\n",
        "A task-specific dataset for sentiment analysis\n",
        "The Hugging Face Transformers and PyTorch libraries\n"
      ],
      "metadata": {
        "id": "5K-BiRVj4Mrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets\n"
      ],
      "metadata": {
        "id": "FgAIKFU54TSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "DNoLR2Ld4af6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Loading a dataset from the Hugging Face Hub (for demonstration purposes)\n",
        "dataset = load_dataset(\"yelp_review_full\", split='train[:10%]')\n",
        "\n",
        "# Tokenizing the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "uk31tsHH4l5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=8)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for batch in train_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "v-q-onZJ49vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It demonstrates how to fine-tune a pre-trained BERT model for a sentiment analysis task. By adjusting the model with more task-specific data, we can improve its ability to understand sentiments, potentially enhancing the performance of layers that initially show low accuracy for this type of task. Fine-tuning allows the model to better adapt to the specifics of sentiment analysis, leveraging the pre-trained knowledge while adjusting to the nuances of the new dataset"
      ],
      "metadata": {
        "id": "lDhmb2gb5iJ8"
      }
    }
  ]
}