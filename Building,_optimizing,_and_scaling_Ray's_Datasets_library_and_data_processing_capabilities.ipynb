{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtHdyL2gYNXSl+6fLZZrc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Building%2C_optimizing%2C_and_scaling_Ray's_Datasets_library_and_data_processing_capabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**building, optimizing, and scaling Ray's Datasets library and data processing capabilities**.\n",
        "The core responsibilities include **improving performance, stability, and integration** with ML workloads,\n",
        "ensuring efficient large-scale data operations.\n",
        "---\n",
        "\n",
        "## **What This Job Will Do**\n",
        "1. **Enhance Ray Datasets Performance**  \n",
        "   - Optimize **Apache Arrow primitives**, **Ray object manager**, and **memory subsystems**.\n",
        "   - Ensure efficient data compaction and large-scale dataset processing.\n",
        "\n",
        "2. **Integrate Ray Data with ML Pipelines**  \n",
        "   - Work with **Ray Train, RLlib, and Serve** to streamline ML workflows.\n",
        "   - Connect data sources efficiently for **large-scale training and inference**.\n",
        "\n",
        "3. **Develop Stability & Stress Testing Infrastructure**  \n",
        "   - Build **robust testing frameworks** to prevent failures at scale.\n",
        "   - Improve **fault tolerance** in distributed environments.\n",
        "\n",
        "4. **Integrate Streaming Workloads into Ray**  \n",
        "   - Work on integrating **Beam on Ray** and **streaming data processing**.\n",
        "\n",
        "5. **Optimize Ray Data for Anyscale Cloud Service**  \n",
        "   - Improve cloud-hosted Ray services by **optimizing distributed data operations**.\n",
        "\n",
        "6. **Contribute to Open Source & Community**  \n",
        "   - Develop **new architectural improvements** for Ray Core and Datasets.\n",
        "   - Write **blogs, tutorials, and talks** to share insights."
      ],
      "metadata": {
        "id": "gos5V5qlEmPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Project Code: Ray Datasets Optimization**\n",
        "This sample project demonstrates how to optimize **Ray Datasets** for large-scale processing using **Apache Arrow and Ray Core**.\n",
        "\n",
        "### **1. Install Ray and Dependencies**"
      ],
      "metadata": {
        "id": "SHYt-gjhDiMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"ray[default]\" \"pyarrow\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GR8in_EUHS",
        "outputId": "4e41edc5-84c1-48e1-dcf6-ecb0f47b56b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Collecting ray[default]\n",
            "  Downloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[default]) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[default]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray[default]) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (4.25.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray[default]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray[default]) (2.32.3)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (3.11.12)\n",
            "Collecting aiohttp-cors (from ray[default])\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default])\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default])\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (2.10.6)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default])\n",
            "  Downloading virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default])\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.70.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (2.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (1.18.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (4.12.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default])\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]) (4.3.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (0.22.3)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]) (1.17.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default]) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.27.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.6.1)\n",
            "Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.29.2-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, aiohttp-cors, ray, opencensus\n",
            "Successfully installed aiohttp-cors-0.7.0 colorful-0.5.6 distlib-0.3.9 opencensus-0.11.4 opencensus-context-0.1.3 py-spy-0.4.0 ray-2.42.1 virtualenv-20.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **2. Basic Ray Dataset Example**\n",
        "import ray\n",
        "import ray.data"
      ],
      "metadata": {
        "id": "NTe_Ga4BEgbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Create a sample dataset\n",
        "ds = ray.data.from_items([{\"id\": i, \"value\": i * 10} for i in range(100)])\n",
        "print(\"Dataset Schema:\", ds.schema())\n",
        "\n",
        "# Apply transformations\n",
        "ds = ds.map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"value\"] * 2})\n",
        "print(\"Transformed Dataset:\", ds.take(5))\n",
        "\n",
        "# Convert to Pandas for ML integration\n",
        "df = ds.to_pandas()\n",
        "print(df.head())\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()"
      ],
      "metadata": {
        "id": "VCz5AKybExTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### **3. Scaling Ray Datasets for Large Data Processing**\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Create a large dataset\n",
        "start = time.time()\n",
        "ds = ray.data.range(1_000_000)\n",
        "ds = ds.map(lambda x: {\"id\": x, \"value\": np.log(x + 1)})\n",
        "#print(\"Processed First 5 Rows:\", ds.take(5))\n",
        "print(\"Processing Time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20i8j6jbE0Sz",
        "outputId": "f590fd2d-5f40-4eab-f019-59d6b7c4e59c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Time: 0.00960850715637207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **4. Distributed Data Loading for ML**\n",
        "import ray.train as train\n",
        "\n",
        "def train_func():\n",
        "    ds = ray.data.from_items([{\"feature\": i, \"label\": i % 2} for i in range(1000)])\n",
        "    for batch in ds.iter_batches(batch_size=10):\n",
        "        print(\"Training on batch:\", batch)\n",
        "\n",
        "train.run(train_func, scaling_config={\"num_workers\": 4})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "yK74g5EYE8OQ",
        "outputId": "322a1fe6-2943-4c1b-f7d6-4ac8bf341e2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'ray.train' has no attribute 'run'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-72d36c39ba0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'ray.train' has no attribute 'run'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### **5. Fault Tolerance & Resiliency Test**\n",
        "```python\n",
        "import ray\n",
        "\n",
        "ray.init()\n",
        "\n",
        "@ray.remote\n",
        "def unstable_task(x):\n",
        "    import random\n",
        "    if random.random() < 0.1:\n",
        "        raise ValueError(\"Random failure!\")\n",
        "    return x * 2\n",
        "\n",
        "# Retry on failure\n",
        "tasks = [unstable_task.remote(i) for i in range(100)]\n",
        "results = ray.get(tasks, ignore_error=True)\n",
        "\n",
        "print(\"Completed tasks:\", len(results))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **How This Job Adapts to the AI/LLM/VLM Era**\n",
        "### **1. AI Model Training at Scale**\n",
        "- Ray Datasets enables **scalable pre-processing for LLMs** (e.g., OpenAI GPT-style training).\n",
        "- Efficient **loading and transformation of massive datasets** for multimodal training.\n",
        "\n",
        "### **2. Optimizing Data Pipelines for Multimodal AI**\n",
        "- **Distributed data processing for VLMs** (Vision-Language Models) using **Ray Data + Apache Arrow**.\n",
        "- Efficient **handling of image, text, and video data**.\n",
        "\n",
        "### **3. Streaming AI Pipelines**\n",
        "- Works on **real-time data ingestion** for reinforcement learning (Ray RLlib).\n",
        "- **Streaming support (Beam on Ray)** for continuous AI model training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts**\n",
        "This role is ideal for those with experience in **distributed systems, data engineering, and AI infrastructure**. If you enjoy **scaling large datasets, optimizing performance, and working on open-source AI infrastructure**, this job aligns well with your expertise.\n",
        "\n",
        "Would you like additional **real-world project ideas** to showcase in an application or interview? ğŸš€"
      ],
      "metadata": {
        "id": "FTmnSOLJE9Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "### **4. Distributed Data Loading for ML**\n",
        "```python\n",
        "import ray.train as train\n",
        "\n",
        "def train_func():\n",
        "    ds = ray.data.from_items([{\"feature\": i, \"label\": i % 2} for i in range(1000)])\n",
        "    for batch in ds.iter_batches(batch_size=10):\n",
        "        print(\"Training on batch:\", batch)\n",
        "\n",
        "train.run(train_func, scaling_config={\"num_workers\": 4})\n",
        "```\n",
        "\n",
        "### **5. Fault Tolerance & Resiliency Test**\n",
        "```python\n",
        "import ray\n",
        "\n",
        "ray.init()\n",
        "\n",
        "@ray.remote\n",
        "def unstable_task(x):\n",
        "    import random\n",
        "    if random.random() < 0.1:\n",
        "        raise ValueError(\"Random failure!\")\n",
        "    return x * 2\n",
        "\n",
        "# Retry on failure\n",
        "tasks = [unstable_task.remote(i) for i in range(100)]\n",
        "results = ray.get(tasks, ignore_error=True)\n",
        "\n",
        "print(\"Completed tasks:\", len(results))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **How This Job Adapts to the AI/LLM/VLM Era**\n",
        "### **1. AI Model Training at Scale**\n",
        "- Ray Datasets enables **scalable pre-processing for LLMs** (e.g., OpenAI GPT-style training).\n",
        "- Efficient **loading and transformation of massive datasets** for multimodal training.\n",
        "\n",
        "### **2. Optimizing Data Pipelines for Multimodal AI**\n",
        "- **Distributed data processing for VLMs** (Vision-Language Models) using **Ray Data + Apache Arrow**.\n",
        "- Efficient **handling of image, text, and video data**.\n",
        "\n",
        "### **3. Streaming AI Pipelines**\n",
        "- Works on **real-time data ingestion** for reinforcement learning (Ray RLlib).\n",
        "- **Streaming support (Beam on Ray)** for continuous AI model training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts**\n",
        "This role is ideal for those with experience in **distributed systems, data engineering, and AI infrastructure**. If you enjoy **scaling large datasets, optimizing performance, and working on open-source AI infrastructure**, this job aligns well with your expertise.\n",
        "\n",
        "Would you like additional **real-world project ideas** to showcase in an application or interview? ğŸš€"
      ],
      "metadata": {
        "id": "VeKdc1IKE8ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "### **4. Distributed Data Loading for ML**\n",
        "```python\n",
        "import ray.train as train\n",
        "\n",
        "def train_func():\n",
        "    ds = ray.data.from_items([{\"feature\": i, \"label\": i % 2} for i in range(1000)])\n",
        "    for batch in ds.iter_batches(batch_size=10):\n",
        "        print(\"Training on batch:\", batch)\n",
        "\n",
        "train.run(train_func, scaling_config={\"num_workers\": 4})\n",
        "```\n",
        "\n",
        "### **5. Fault Tolerance & Resiliency Test**\n",
        "```python\n",
        "import ray\n",
        "\n",
        "ray.init()\n",
        "\n",
        "@ray.remote\n",
        "def unstable_task(x):\n",
        "    import random\n",
        "    if random.random() < 0.1:\n",
        "        raise ValueError(\"Random failure!\")\n",
        "    return x * 2\n",
        "\n",
        "# Retry on failure\n",
        "tasks = [unstable_task.remote(i) for i in range(100)]\n",
        "results = ray.get(tasks, ignore_error=True)\n",
        "\n",
        "print(\"Completed tasks:\", len(results))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **How This Job Adapts to the AI/LLM/VLM Era**\n",
        "### **1. AI Model Training at Scale**\n",
        "- Ray Datasets enables **scalable pre-processing for LLMs** (e.g., OpenAI GPT-style training).\n",
        "- Efficient **loading and transformation of massive datasets** for multimodal training.\n",
        "\n",
        "### **2. Optimizing Data Pipelines for Multimodal AI**\n",
        "- **Distributed data processing for VLMs** (Vision-Language Models) using **Ray Data + Apache Arrow**.\n",
        "- Efficient **handling of image, text, and video data**.\n",
        "\n",
        "### **3. Streaming AI Pipelines**\n",
        "- Works on **real-time data ingestion** for reinforcement learning (Ray RLlib).\n",
        "- **Streaming support (Beam on Ray)** for continuous AI model training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts**\n",
        "This role is ideal for those with experience in **distributed systems, data engineering, and AI infrastructure**. If you enjoy **scaling large datasets, optimizing performance, and working on open-source AI infrastructure**, this job aligns well with your expertise.\n",
        "\n",
        "Would you like additional **real-world project ideas** to showcase in an application or interview? ğŸš€"
      ],
      "metadata": {
        "id": "Ms7frhS6DGE1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezD4tAqPEeAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}