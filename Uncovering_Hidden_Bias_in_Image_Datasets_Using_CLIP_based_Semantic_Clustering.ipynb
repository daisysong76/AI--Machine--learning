{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFjrnqXbij7tR0TIVScsQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Uncovering_Hidden_Bias_in_Image_Datasets_Using_CLIP_based_Semantic_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Uncovering Hidden Bias in Image Datasets Using CLIP-based Semantic Clustering\"\n",
        "\n",
        "üß† Project Goal:\n",
        "Use CLIP embeddings to cluster images based on semantic content, then detect imbalances, stereotype reinforcement, or redundancy in large-scale image datasets ‚Äî e.g., image-caption pairs for training or evaluating vision-language models.\n",
        "\n",
        "üí° Use Case Examples:\n",
        "Stock photo datasets (e.g., Unsplash, OpenImages): Do all ‚Äúdoctors‚Äù appear as men?\n",
        "\n",
        "Social media image posts: Are certain activities (e.g., cooking, driving) associated with only one gender or ethnicity?\n",
        "\n",
        "LLM-generated images: Do generative models output biased visual stereotypes?\n",
        "\n",
        "üß∞ Tools & Libraries:\n",
        "ü§ñ CLIP (OpenAI or HuggingFace version)\n",
        "\n",
        "üìä scikit-learn (for KMeans or DBSCAN clustering)\n",
        "\n",
        "üìà UMAP/t-SNE (for visualization)\n",
        "\n",
        "üñºÔ∏è Matplotlib / Plotly (interactive visuals)\n",
        "\n",
        "üß† (Optional) DINOv2 or BLIP for comparison\n",
        "\n",
        "‚úÖ Workflow (with Code Skeletons):\n",
        "1. Load & Preprocess Images\n"
      ],
      "metadata": {
        "id": "faCEokbCNff5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image_folder = \"data/images\"\n",
        "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
        "\n",
        "images = [preprocess(Image.open(path)).unsqueeze(0).to(device) for path in image_paths]\n",
        "image_tensor = torch.cat(images, dim=0)"
      ],
      "metadata": {
        "id": "yXWpo5MrNpyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Generate CLIP Embeddings"
      ],
      "metadata": {
        "id": "S4WYdIFNNrvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_tensor).cpu().numpy()"
      ],
      "metadata": {
        "id": "_GIx-4JPNxYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cluster Images by Semantic Similarity"
      ],
      "metadata": {
        "id": "jqRdYfTyNzKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 10\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels = kmeans.fit_predict(image_features)"
      ],
      "metadata": {
        "id": "WAZunPVqN2AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Visualize Clusters"
      ],
      "metadata": {
        "id": "-aA3UsbPN3hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30)\n",
        "projected = tsne.fit_transform(image_features)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(projected[:, 0], projected[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"CLIP-based Image Clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lTga9nLHN7LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Investigate Bias or Imbalance\n",
        "Manually inspect samples from each cluster:"
      ],
      "metadata": {
        "id": "J1LDWt7iN9I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    cluster_folder = f\"clusters/cluster_{cluster_id}\"\n",
        "    os.makedirs(cluster_folder, exist_ok=True)\n",
        "    for idx, label in enumerate(labels):\n",
        "        if label == cluster_id:\n",
        "            shutil.copy(image_paths[idx], os.path.join(cluster_folder, os.path.basename(image_paths[idx])))"
      ],
      "metadata": {
        "id": "vMuv8iMXOAVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then open the folders. Are some clusters dominated by certain demographics, contexts, or aesthetics?\n",
        "\n",
        "üìà Bonus Ideas for Expansion:\n",
        "Pair with image captions and check language patterns (e.g., are certain clusters described with gendered or emotional language?)\n",
        "\n",
        "Run Diversity Scores: compute intra-cluster visual diversity\n",
        "\n",
        "Use this pipeline to clean, rebalance, or augment datasets for fine-tuning VLMs"
      ],
      "metadata": {
        "id": "zbl1Eoo0OCbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚ÄúI used CLIP-based clustering to surface latent semantic patterns in multimodal training data. For example, I found that certain clusters overrepresented Western-centric business settings when prompted with 'CEO'. Based on this, I proposed rebalancing samples and flagged stereotype-enforcing patterns before model fine-tuning.‚Äù"
      ],
      "metadata": {
        "id": "TBipl8JXOL6G"
      }
    }
  ]
}