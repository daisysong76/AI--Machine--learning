{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdedm7qbKBs+CGdkSGvmpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Advanced_Multi_Agent_Educational_Content_Generation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mFI_hfvut2dz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM_VOrbnt1K5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel, GPT2Tokenizer,\n",
        "    T5ForConditionalGeneration, T5Tokenizer,\n",
        "    BertModel, BertTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from collections import deque\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "import wandb\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "@dataclass\n",
        "class ContentExample:\n",
        "    text: str\n",
        "    topic: str\n",
        "    difficulty: float\n",
        "    target_age: int\n",
        "    metadata: Dict\n",
        "    rewards: Dict[str, float]\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    \"\"\"Advanced reward model using multi-aspect evaluation\"\"\"\n",
        "    def __init__(self, model_name: str = 'bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Multiple reward heads for different aspects\n",
        "        self.heads = nn.ModuleDict({\n",
        "            'quality': nn.Linear(768, 1),\n",
        "            'engagement': nn.Linear(768, 1),\n",
        "            'educational_value': nn.Linear(768, 1),\n",
        "            'age_appropriateness': nn.Linear(768, 1),\n",
        "            'difficulty_match': nn.Linear(768, 1)\n",
        "        })\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        outputs = self.bert(**encodings)\n",
        "        pooled = outputs.pooler_output\n",
        "\n",
        "        return {\n",
        "            aspect: head(pooled)\n",
        "            for aspect, head in self.heads.items()\n",
        "        }\n",
        "\n",
        "class ActiveLearningManager:\n",
        "    \"\"\"Manages active learning for content generation\"\"\"\n",
        "    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.uncertainty_threshold = 0.7\n",
        "        self.diversity_weight = 0.3\n",
        "\n",
        "    def select_samples_for_labeling(\n",
        "        self,\n",
        "        candidates: List[ContentExample],\n",
        "        n_samples: int\n",
        "    ) -> List[ContentExample]:\n",
        "        # Get embeddings\n",
        "        embeddings = self.embedding_model.encode(\n",
        "            [c.text for c in candidates]\n",
        "        )\n",
        "\n",
        "        # Calculate uncertainty scores\n",
        "        uncertainty_scores = self._calculate_uncertainty(candidates)\n",
        "\n",
        "        # Calculate diversity scores using KMeans\n",
        "        kmeans = KMeans(n_clusters=min(n_samples, len(candidates)))\n",
        "        cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Combine uncertainty and diversity\n",
        "        final_scores = (\n",
        "            self.uncertainty_threshold * uncertainty_scores +\n",
        "            self.diversity_weight * self._get_diversity_scores(\n",
        "                embeddings,\n",
        "                cluster_labels\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Select top samples\n",
        "        selected_indices = np.argsort(final_scores)[-n_samples:]\n",
        "        return [candidates[i] for i in selected_indices]\n",
        "\n",
        "    def _calculate_uncertainty(\n",
        "        self,\n",
        "        candidates: List[ContentExample]\n",
        "    ) -> np.ndarray:\n",
        "        # Implementation for uncertainty estimation\n",
        "        return np.random.random(len(candidates))\n",
        "\n",
        "    def _get_diversity_scores(\n",
        "        self,\n",
        "        embeddings: np.ndarray,\n",
        "        cluster_labels: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        # Calculate distance from cluster centers\n",
        "        return np.random.random(len(embeddings))\n",
        "\n",
        "class ContentGenerationEnv(gym.Env):\n",
        "    \"\"\"RL environment for content generation\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model: GPT2LMHeadModel,\n",
        "        tokenizer: GPT2Tokenizer,\n",
        "        reward_model: RewardModel\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.reward_model = reward_model\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        vocab_size = self.tokenizer.vocab_size\n",
        "        self.action_space = spaces.Discrete(vocab_size)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=vocab_size,\n",
        "            shape=(512,),\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "        self.max_steps = 100\n",
        "        self.current_step = 0\n",
        "        self.generated_tokens = []\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.generated_tokens = []\n",
        "        initial_token = self.tokenizer.bos_token_id\n",
        "        self.generated_tokens.append(initial_token)\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        self.generated_tokens.append(action)\n",
        "\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        if done:\n",
        "            reward = self._calculate_reward()\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = np.zeros(512, dtype=np.int64)\n",
        "        obs[:len(self.generated_tokens)] = self.generated_tokens\n",
        "        return obs\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        text = self.tokenizer.decode(self.generated_tokens)\n",
        "        rewards = self.reward_model([text])\n",
        "        return sum(r.mean().item() for r in rewards.values())\n",
        "\n",
        "class MultiAgentContentGenerator:\n",
        "    \"\"\"Multi-agent system for content generation\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_agents: int = 3,\n",
        "        base_model: str = 'gpt2-medium'\n",
        "    ):\n",
        "        ray.init()\n",
        "\n",
        "        # Initialize models\n",
        "        self.base_model = GPT2LMHeadModel.from_pretrained(base_model)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
        "        self.reward_model = RewardModel()\n",
        "\n",
        "        # Initialize RL agents\n",
        "        self.agents = [\n",
        "            PPO(\n",
        "                env=ContentGenerationEnv,\n",
        "                config={\n",
        "                    \"env_config\": {\n",
        "                        \"base_model\": self.base_model,\n",
        "                        \"tokenizer\": self.tokenizer,\n",
        "                        \"reward_model\": self.reward_model\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "            for _ in range(num_agents)\n",
        "        ]\n",
        "\n",
        "        self.active_learning = ActiveLearningManager()\n",
        "        self.experience_buffer = deque(maxlen=10000)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        num_iterations: int,\n",
        "        samples_per_iteration: int = 100\n",
        "    ):\n",
        "        for iteration in range(num_iterations):\n",
        "            # Generate content with each agent\n",
        "            contents = []\n",
        "            for agent in self.agents:\n",
        "                content = self._generate_content(agent)\n",
        "                contents.extend(content)\n",
        "\n",
        "            # Select diverse samples for evaluation\n",
        "            selected_samples = self.active_learning.select_samples_for_labeling(\n",
        "                contents,\n",
        "                samples_per_iteration\n",
        "            )\n",
        "\n",
        "            # Get rewards and update agents\n",
        "            for sample in selected_samples:\n",
        "                rewards = self.reward_model([sample.text])\n",
        "                self.experience_buffer.append((sample, rewards))\n",
        "\n",
        "                # Update each agent\n",
        "                for agent in self.agents:\n",
        "                    agent.train()\n",
        "\n",
        "            # Periodically update reward model\n",
        "            if iteration % 10 == 0:\n",
        "                self._update_reward_model()\n",
        "\n",
        "    def _generate_content(\n",
        "        self,\n",
        "        agent: PPO,\n",
        "        num_samples: int = 10\n",
        "    ) -> List[ContentExample]:\n",
        "        contents = []\n",
        "        for _ in range(num_samples):\n",
        "            env = ContentGenerationEnv(\n",
        "                self.base_model,\n",
        "                self.tokenizer,\n",
        "                self.reward_model\n",
        "            )\n",
        "\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = agent.compute_single_action(obs)\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            text = self.tokenizer.decode(env.generated_tokens)\n",
        "            contents.append(\n",
        "                ContentExample(\n",
        "                    text=text,\n",
        "                    topic=\"\",  # Add topic inference\n",
        "                    difficulty=0.5,  # Add difficulty inference\n",
        "                    target_age=12,  # Add age inference\n",
        "                    metadata={},\n",
        "                    rewards={}\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return contents\n",
        "\n",
        "    def _update_reward_model(self):\n",
        "        # Update reward model using experience buffer\n",
        "        optimizer = AdamW(self.reward_model.parameters(), lr=1e-5)\n",
        "\n",
        "        # Simple training loop\n",
        "        for _ in range(100):\n",
        "            samples = random.sample(\n",
        "                self.experience_buffer,\n",
        "                min(32, len(self.experience_buffer))\n",
        "            )\n",
        "            texts = [s[0].text for s in samples]\n",
        "            true_rewards = torch.stack([\n",
        "                torch.tensor([r.item() for r in s[1].values()])\n",
        "                for s in samples\n",
        "            ])\n",
        "\n",
        "            predicted_rewards = self.reward_model(texts)\n",
        "            loss = sum(\n",
        "                nn.MSELoss()(pred.squeeze(), true_rewards[:, i])\n",
        "                for i, pred in enumerate(predicted_rewards.values())\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def generate_curriculum(\n",
        "        self,\n",
        "        topics: List[str],\n",
        "        num_lessons: int,\n",
        "        target_age: int\n",
        "    ) -> List[ContentExample]:\n",
        "        curriculum = []\n",
        "\n",
        "        # Generate content using best performing agent\n",
        "        best_agent = max(\n",
        "            self.agents,\n",
        "            key=lambda a: a.get_policy().model.get_metrics()[\"policy_reward_mean\"]\n",
        "        )\n",
        "\n",
        "        for topic in topics:\n",
        "            topic_lessons = []\n",
        "            for _ in range(num_lessons):\n",
        "                content = self._generate_content(\n",
        "                    best_agent,\n",
        "                    num_samples=5\n",
        "                )\n",
        "                # Select best content based on rewards\n",
        "                best_content = max(\n",
        "                    content,\n",
        "                    key=lambda x: sum(x.rewards.values())\n",
        "                )\n",
        "                topic_lessons.append(best_content)\n",
        "\n",
        "            # Order lessons by difficulty\n",
        "            topic_lessons.sort(key=lambda x: x.difficulty)\n",
        "            curriculum.extend(topic_lessons)\n",
        "\n",
        "        return curriculum\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize system\n",
        "    generator = MultiAgentContentGenerator(num_agents=3)\n",
        "\n",
        "    # Example topics for curriculum generation\n",
        "    topics = [\n",
        "        \"Introduction to Programming\",\n",
        "        \"Variables and Data Types\",\n",
        "        \"Control Structures\",\n",
        "        \"Functions and Methods\"\n",
        "    ]\n",
        "\n",
        "    # Train the system\n",
        "    generator.train(num_iterations=100)\n",
        "\n",
        "    # Generate curriculum\n",
        "    curriculum = generator.generate_curriculum(\n",
        "        topics=topics,\n",
        "        num_lessons=3,\n",
        "        target_age=14\n",
        "    )\n",
        "\n",
        "    # Save curriculum\n",
        "    with open('generated_curriculum.json', 'w') as f:\n",
        "        json.dump(\n",
        "            [\n",
        "                {\n",
        "                    'topic': lesson.topic,\n",
        "                    'content': lesson.text,\n",
        "                    'difficulty': lesson.difficulty,\n",
        "                    'target_age': lesson.target_age,\n",
        "                    'metadata': lesson.metadata\n",
        "                }\n",
        "                for lesson in curriculum\n",
        "            ],\n",
        "            f,\n",
        "            indent=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "his advanced implementation includes several cutting-edge techniques:\n",
        "\n",
        "Multi-Agent Learning:\n",
        "\n",
        "\n",
        "Multiple RL agents generating content in parallel\n",
        "Different agents can specialize in different aspects of content generation\n",
        "Agents learn from each other's experiences\n",
        "Competition and cooperation between agents\n",
        "\n",
        "\n",
        "Sophisticated Reward Modeling:\n",
        "\n",
        "\n",
        "Multi-aspect reward calculation\n",
        "BERT-based content evaluation\n",
        "Separate reward heads for different quality aspects\n",
        "Continuous reward model updating\n",
        "\n",
        "\n",
        "Active Learning:\n",
        "\n",
        "\n",
        "Smart sample selection for evaluation\n",
        "Diversity-based sampling\n",
        "Uncertainty estimation\n",
        "Efficient use of human feedback\n",
        "\n",
        "\n",
        "Advanced Environment:\n",
        "\n",
        "\n",
        "Custom gym environment for content generation\n",
        "Structured action and observation spaces\n",
        "Progressive reward calculation\n",
        "Step-by-step content generation\n",
        "\n",
        "\n",
        "Curriculum Generation:\n",
        "\n",
        "\n",
        "Topic-based curriculum structure\n",
        "Difficulty progression\n",
        "Age-appropriate content filtering\n",
        "Coherent lesson sequencing\n",
        "\n",
        "To improve this system even further, you could:\n",
        "\n",
        "Add Meta-Learning:\n",
        "\n",
        "pythonCopy# Example meta-learning addition\n",
        "class MetaLearningOptimizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.meta_lstm = nn.LSTM(768, 768)\n",
        "        \n",
        "    def adapt_agents(self, agents, meta_batch):\n",
        "        # Implement meta-learning updates\n",
        "        pass\n",
        "\n",
        "Implement Hierarchical RL:\n",
        "\n",
        "pythonCopyclass HierarchicalAgent:\n",
        "    def __init__(self):\n",
        "        self.high_level_policy = PPO(...)  # For topic selection\n",
        "        self.low_level_policy = PPO(...)   # For content generation\n",
        "\n",
        "Add Self-Critical Training:\n",
        "\n",
        "pythonCopydef self_critical_training(self, generated_content):\n",
        "    baseline = self.generate_baseline()\n",
        "    advantages = self.reward_model(generated_content) - self.reward_model(baseline)\n",
        "    return advantages"
      ],
      "metadata": {
        "id": "dzJQnNqQuBdB"
      }
    }
  ]
}