{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Custom_Kernel_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# --- Custom CUDA Kernel Implementation ---\n",
        "cuda_kernel = \"\"\"\n",
        "extern \"C\" __global__ void custom_sin_kernel(float *input, float *output, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = sinf(input[idx]);\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cpp_wrapper = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "void custom_sin_cuda(torch::Tensor input, torch::Tensor output) {\n",
        "    int size = input.numel();\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (size + blockSize - 1) / blockSize;\n",
        "\n",
        "    custom_sin_kernel<<<gridSize, blockSize>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"forward\", &custom_sin_cuda, \"Custom sin CUDA kernel\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "custom_sin_module = load_inline(\n",
        "    name=\"custom_sin_module\",\n",
        "    cpp_sources=[cpp_wrapper],\n",
        "    cuda_sources=[cuda_kernel],\n",
        "    functions=['forward'],\n",
        "    extra_cuda_cflags=['-lcudart'],\n",
        "    verbose=False  # Set to True for debugging compilation\n",
        ")\n",
        "\n",
        "class CustomSin(nn.Module):\n",
        "    def forward(self, x):\n",
        "        output = torch.empty_like(x)\n",
        "        custom_sin_module.forward(x, output)\n",
        "        return output\n",
        "# --- End Custom CUDA Kernel Implementation ---\n",
        "\n",
        "# Simulated Model (Simple Linear for demonstration)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(128, output_size)\n",
        "        self.custom_sin = CustomSin() # Add the custom sin layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.custom_sin(x) # Use the custom sin layer\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Simulated Data\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "batch_size = 64\n",
        "data_size = 1000\n",
        "\n",
        "data = torch.randn(data_size, input_size).cuda()\n",
        "labels = torch.randn(data_size, output_size).cuda()\n",
        "\n",
        "dataset = TensorDataset(data, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, Optimizer, Loss\n",
        "model = SimpleModel(input_size, output_size).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Mixed Precision Training with GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Gradient Accumulation (Simulated, accumulate every 4 batches)\n",
        "accumulation_steps = 4\n",
        "\n",
        "# Gradient Checkpointing (Simulated, using a dummy function, in real cases, use torch.utils.checkpoint.checkpoint)\n",
        "def checkpoint_dummy(func, *inputs):\n",
        "    return func(*inputs)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "        with autocast(): # Enables mixed precision\n",
        "            # Simulated Kernel Fusion (combining relu and linear2 as a conceptual example)\n",
        "            x = model.linear1(inputs)\n",
        "            x = checkpoint_dummy(model.relu, x) # Simulated Gradient Checkpointing\n",
        "            outputs = model(x) # Model now uses the custom sin kernel\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss = loss / accumulation_steps # Normalize loss for gradient accumulation\n",
        "\n",
        "        scaler.scale(loss).backward() # Scaled backward pass\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer) # Update weights, unscale gradients\n",
        "            scaler.update() # Updates scale for next iteration\n",
        "            optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item() * accumulation_steps:.4f}\")\n",
        "\n",
        "# Simulated Optimizer State Offloading (Conceptual)\n",
        "# In a real scenario, you would move optimizer states to CPU memory.\n",
        "# Example (conceptual):\n",
        "# optimizer.state['exp_avg'].cpu() # Moving a state to CPU.\n",
        "\n",
        "# Simulated Smart Prefetching (Conceptual)\n",
        "# In real scenarios, you would use DataLoader's prefetch_factor or custom data loading logic.\n",
        "# Example (conceptual):\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, prefetch_factor=4)\n",
        "\n",
        "print(\"Training finished!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5qsUKkuugicD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Changes:**\n",
        "\n",
        "1.  **Custom Kernel Integration:**\n",
        "    * The CUDA kernel and C++ wrapper code are included at the beginning of the script.\n",
        "    * The `CustomSin` `nn.Module` is created to wrap the custom kernel.\n",
        "    * An instance of `CustomSin` is added as a layer within the `SimpleModel`.\n",
        "    * The `forward` method of `SimpleModel` is modified to call the `CustomSin` layer.\n",
        "\n",
        "2.  **Model Forward Pass:**\n",
        "    * The `model(x)` call in the training loop now includes the execution of the custom CUDA kernel.\n",
        "\n",
        "3.  **Compilation:**\n",
        "    * The `load_inline` function compiles the custom kernel at runtime. If you encounter compilation issues, set `verbose=True` in `load_inline` to see the compiler output.\n",
        "\n",
        "**How This Works:**\n",
        "\n",
        "* The custom CUDA kernel performs the sine operation on the GPU, potentially offering performance benefits compared to the standard PyTorch `torch.sin()`, especially for large tensors.\n",
        "* By integrating the custom kernel as a layer in the model, you can seamlessly use it within your PyTorch training pipeline.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Compilation Time:** Compiling the custom kernel can take some time, especially on the first run.\n",
        "* **Error Handling:** The provided code has minimal error handling. In real-world applications, you should add error handling to the CUDA kernel and C++ wrapper.\n",
        "* **Performance Measurement:** To evaluate the performance benefits of the custom kernel, you should benchmark it against the standard PyTorch `torch.sin()` function using large tensors.\n",
        "* **Debugging:** Debugging CUDA kernels can be challenging. You can use tools like `cuda-gdb` or `Nsight Systems` to debug your kernels.\n",
        "* **Real World complexity:** In a real world scenario, the custom kernel would be doing far more complex and useful operations than a simple sin function."
      ],
      "metadata": {
        "id": "42j75Lq_gicN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/STomoya/animeface\">https://github.com/STomoya/animeface</a> subject to MIT</li>\n",
        "  <li><a href=\"https://discuss.pytorch.org/t/quantizer-backend-for-linear-op-intermittent-failures-executorch/202318\">https://discuss.pytorch.org/t/quantizer-backend-for-linear-op-intermittent-failures-executorch/202318</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "pvGr2uNzgicT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}