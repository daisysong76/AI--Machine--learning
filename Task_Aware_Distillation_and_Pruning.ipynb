{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoAMdW/SW8jbBZGUy4dZBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/Task_Aware_Distillation_and_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-Aware Distillation and Pruning with a Transformer Model (BERT)\n",
        "Here, we'll use Hugging Face’s DistilBERT as an example of model distillation and torch.nn.utils.prune for pruning. Suppose we're distilling and pruning a BERT model for a text classification task on a custom dataset.\n",
        "\n",
        "1. Distillation: Train a DistilBERT model on a task-specific dataset using knowledge distillation from a pre-trained BERT model.\n",
        "2. Pruning: Prune the smaller DistilBERT model for further efficiency.\n",
        "Step 1: Set Up Libraries"
      ],
      "metadata": {
        "id": "B2R1VuAw87m2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSXo2w_wdXgi",
        "outputId": "269f7f80-631d-47a2-bb24-3d29910d4f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting dataset\n",
            "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
            "  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting alembic>=0.6.2 (from dataset)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting banal>=1.0.1 (from dataset)\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting Mako (from alembic>=0.6.2->dataset)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
            "Downloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: banal, sqlalchemy, Mako, alembic, dataset\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 alembic-1.13.3 banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.54\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sample dataset for text classification\n",
        "dataset = load_dataset(\"imdb\")\n"
      ],
      "metadata": {
        "id": "yEBHRdzN9MQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Teacher Model (BERT) and Student Model (DistilBERT)"
      ],
      "metadata": {
        "id": "JKZQy4l69QyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT as the teacher model\n",
        "teacher_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load DistilBERT as the student model\n",
        "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "YHtAOpqL9Rmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Distillation Training Loop\n",
        "\n",
        "Define the training arguments and set up the Trainer for knowledge distillation. The student model will learn by matching its logits with the teacher's logits."
      ],
      "metadata": {
        "id": "WRiCuhPu9TvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "# Define a custom distillation loss function\n",
        "def distillation_loss(student_outputs, teacher_outputs, labels, alpha=0.5, temperature=2.0):\n",
        "    soft_teacher_logits = F.log_softmax(teacher_outputs.logits / temperature, dim=-1)\n",
        "    soft_student_logits = F.log_softmax(student_outputs.logits / temperature, dim=-1)\n",
        "\n",
        "    # Cross-entropy between soft labels\n",
        "    distill_loss = F.kl_div(soft_student_logits, soft_teacher_logits, reduction=\"batchmean\") * (temperature ** 2)\n",
        "\n",
        "    # Cross-entropy with ground truth labels\n",
        "    hard_loss = F.cross_entropy(student_outputs.logits, labels)\n",
        "\n",
        "    # Combine losses with weighting factor alpha\n",
        "    return alpha * distill_loss + (1 - alpha) * hard_loss\n"
      ],
      "metadata": {
        "id": "5uJpgoES9Y4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom Trainer class to use the distillation loss:"
      ],
      "metadata": {
        "id": "LkcjonoSf-Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Get teacher model predictions for distillation\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(**inputs)\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        loss = distillation_loss(outputs, teacher_outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "KfKvdDgSf4ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tune the Distilled Model"
      ],
      "metadata": {
        "id": "shlnUPc1f88y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilled_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "trainer = DistillationTrainer(\n",
        "    model=student_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"]\n",
        ")\n",
        "\n",
        "# Start distillation training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "h0KUQVl5gDq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTXhLZifgqCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prune the Fine-Tuned Model\n",
        "\n",
        "Once the student model has been fine-tuned, we’ll apply pruning. Here, we’ll prune the linear layers in DistilBERT by removing low-magnitude weights."
      ],
      "metadata": {
        "id": "c2bU6BvSgVsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Function to apply pruning to all linear layers\n",
        "def apply_pruning(model, pruning_amount=0.3):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=pruning_amount)\n",
        "            prune.remove(module, 'weight')  # Make pruning permanent\n",
        "\n",
        "# Apply pruning to the student model\n",
        "apply_pruning(student_model, pruning_amount=0.3)\n"
      ],
      "metadata": {
        "id": "zRg3zGpsgadA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save and Evaluate the Pruned Model\n",
        "After pruning, save and evaluate the model on the test set to ensure it performs well despite the reduced size."
      ],
      "metadata": {
        "id": "KeKQqZixglul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pruned model\n",
        "student_model.save_pretrained(\"./pruned_distilled_model\")\n",
        "\n",
        "# Evaluate the pruned model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Pruned Model Evaluation Results: {eval_results}\")\n"
      ],
      "metadata": {
        "id": "vc4vMjX4gmam"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}