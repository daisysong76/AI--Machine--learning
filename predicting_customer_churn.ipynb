{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPREnYdQrTshW4VzTbHGHQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/predicting_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a project aimed at predicting customer churn, I was initially working with a logistic regression model. The initial results were promising but not up to the desired accuracy level, and the model also struggled with slow prediction times when deployed. To optimize the model, I employed several strategies:\n",
        "Feature Engineering: I revisited the feature selection and extraction process to ensure that the model was receiving the most relevant information. This involved removing redundant features, creating interaction terms, and applying principal component analysis (PCA) to reduce dimensionality while retaining the variance in the dataset.\n",
        "Hyperparameter Tuning: I used grid search with cross-validation to systematically explore a wide range of hyperparameters for the logistic regression to find the optimal settings. This helped in improving the modelâ€™s accuracy significantly.\n",
        "Model Selection: Realizing that logistic regression might be too simplistic for the complexity of the data, I tested several other algorithms, including Random Forest and Gradient Boosting Machines (GBM). The GBM outperformed other models in terms of both accuracy and execution speed in the production environment.\n",
        "Ensemble Methods: To further enhance the performance, I employed a stacking ensemble method that combined the predictions from logistic regression, Random Forest, and GBM. This approach leveraged the strengths of each model and improved the overall prediction accuracy.\n",
        "Post-processing Techniques: I implemented calibration techniques to adjust the probability outputs from the model, which helped in improving the reliability of the predictions.\n"
      ],
      "metadata": {
        "id": "5SmziYbIWaxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOoNi2dvWXUm"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load and Preprocess Data\n",
        "data = pd.read_csv('path/to/your/dataset.csv')\n",
        "X = data.drop(columns=['target'])  # Replace 'target' with your dependent variable\n",
        "y = data['target']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Engineering\n",
        "# Example: PCA for Dimensionality Reduction\n",
        "pca = PCA(n_components=10)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Hyperparameter Tuning for Logistic Regression\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "logistic = LogisticRegression()\n",
        "grid_lr = GridSearchCV(logistic, param_grid_lr, cv=5, scoring='accuracy')\n",
        "grid_lr.fit(X_train_pca, y_train)\n",
        "\n",
        "# Best Logistic Regression Model\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "# Random Forest and GBM Model Selection\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "random_forest.fit(X_train_pca, y_train)\n",
        "gbm.fit(X_train_pca, y_train)\n",
        "\n",
        "# Stacking Ensemble Model\n",
        "estimators = [\n",
        "    ('lr', best_lr),\n",
        "    ('rf', random_forest),\n",
        "    ('gbm', gbm)\n",
        "]\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Calibrating the Model\n",
        "calibrated_model = CalibratedClassifierCV(base_estimator=stacking_model, method='sigmoid')\n",
        "calibrated_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = calibrated_model.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_report_output = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report_output)\n"
      ]
    }
  ]
}