{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNds+XzfYhiW9SR0aXPzOP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI--Machine--learning/blob/main/DiaTweet_Advanced_Algorithm_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class AdvancedDiabetesTweetAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Sentiment Analysis Model\n",
        "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "        )\n",
        "\n",
        "        # NER Model\n",
        "        self.nlp = spacy.load('en_core_web_trf')\n",
        "\n",
        "        # TF-IDF Vectorizer\n",
        "        self.tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "    def analyze_sentiment(self, tweet):\n",
        "        \"\"\"Advanced sentiment analysis with confidence scoring\"\"\"\n",
        "        inputs = self.tokenizer(tweet, return_tensors='pt')\n",
        "        outputs = self.sentiment_model(**inputs)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        return {\n",
        "            'sentiment': torch.argmax(probabilities).item(),\n",
        "            'confidence': torch.max(probabilities).item()\n",
        "        }\n",
        "\n",
        "    def extract_entities(self, tweet):\n",
        "        \"\"\"Enhanced Named Entity Recognition\"\"\"\n",
        "        doc = self.nlp(tweet)\n",
        "        return [\n",
        "            {\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'diabetes_relevance': self._calculate_diabetes_relevance(ent)\n",
        "            } for ent in doc.ents\n",
        "        ]\n",
        "\n",
        "    def _calculate_diabetes_relevance(self, entity):\n",
        "        \"\"\"Custom relevance scoring for medical entities\"\"\"\n",
        "        diabetes_keywords = [\n",
        "            'insulin', 'glucose', 'blood sugar',\n",
        "            'CGM', 'A1C', 'diabetes'\n",
        "        ]\n",
        "        return any(keyword in entity.text.lower() for keyword in diabetes_keywords)\n",
        "\n",
        "    def extract_features(self, tweets):\n",
        "        \"\"\"TF-IDF feature extraction\"\"\"\n",
        "        return self.tfidf.fit_transform(tweets)\n",
        "\n",
        "# Usage Example\n",
        "analyzer = AdvancedDiabetesTweetAnalyzer()\n",
        "tweet = \"Managing my diabetes with a new insulin pump! #diabetes #health\"\n",
        "\n",
        "sentiment = analyzer.analyze_sentiment(tweet)\n",
        "entities = analyzer.extract_entities(tweet)\n",
        "\n",
        "print(\"Sentiment:\", sentiment)\n",
        "print(\"Entities:\", entities)"
      ],
      "metadata": {
        "id": "dY-MKOJL2_Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DiaTweet: Advanced Algorithm Selection\n",
        "\n",
        "## Sentiment Analysis Strategy\n",
        "1. **Primary Model**: RoBERTa fine-tuned on healthcare/Twitter data\n",
        "   - High accuracy in understanding context\n",
        "   - Handles noisy, domain-specific text\n",
        "   - Low computational overhead\n",
        "\n",
        "2. **Ensemble Approach**\n",
        "   - Combine RoBERTa with rule-based sentiment scoring\n",
        "   - Create a confidence-weighted sentiment classifier\n",
        "   - Improve overall sentiment detection accuracy\n",
        "\n",
        "## Named Entity Recognition (NER)\n",
        "1. **Transformer-based SpaCy Model**\n",
        "   - State-of-the-art performance\n",
        "   - Pre-trained on medical/health terminology\n",
        "   - Efficient entity extraction\n",
        "\n",
        "2. **Custom Rule-based Extension**\n",
        "   - Add domain-specific diabetes-related entity patterns\n",
        "   - Enhance SpaCy's default entity recognition\n",
        "   - Capture nuanced medical terminology\n",
        "\n",
        "## Text Classification Pipeline\n",
        "1. **Multi-stage Classification**\n",
        "   - Initial coarse-grained classification (Naive Bayes)\n",
        "   - Fine-grained classification (Transformer models)\n",
        "   - Hierarchical classification approach\n",
        "\n",
        "## Feature Extraction\n",
        "1. **Hybrid Embedding Approach**\n",
        "   - Word2Vec for general semantic understanding\n",
        "   - Domain-specific embeddings for medical context\n",
        "   - TF-IDF for keyword importance\n",
        "\n",
        "## Performance Optimization\n",
        "- Use lightweight models (DistilBERT, MiniLM)\n",
        "- Implement caching mechanisms\n",
        "- Leverage GPU acceleration\n",
        "- Batch processing of tweets"
      ],
      "metadata": {
        "id": "RwXSVYcX3L5p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "collapsed": true,
        "id": "2LkFW7zE07p-",
        "outputId": "c6251ff2-37c1-422e-d4e7-59eecbe6281a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'elasticsearch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-754a9c969d5e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0melasticsearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'elasticsearch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# Advanced Imports\n",
        "import transformers\n",
        "import torch\n",
        "import spacy\n",
        "import elasticsearch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Any\n",
        "\n",
        "# Microsoft Cognitive Services Integration\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "class DiaTweetAnalyzer:\n",
        "    def __init__(self,\n",
        "                 twitter_api_key: str,\n",
        "                 azure_endpoint: str,\n",
        "                 azure_key: str,\n",
        "                 es_host: str = 'localhost'):\n",
        "        \"\"\"\n",
        "        Initialize DiaTweet advanced analysis pipeline\n",
        "\n",
        "        Args:\n",
        "            twitter_api_key (str): Twitter API authentication\n",
        "            azure_endpoint (str): Azure Cognitive Services endpoint\n",
        "            azure_key (str): Azure service key\n",
        "            es_host (str): Elasticsearch host\n",
        "        \"\"\"\n",
        "        # Logging configuration\n",
        "        logging.basicConfig(level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s: %(message)s')\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Azure Text Analytics Client\n",
        "        self.text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=azure_endpoint,\n",
        "            credential=AzureKeyCredential(azure_key)\n",
        "        )\n",
        "\n",
        "        # Advanced NLP Models\n",
        "        self.nlp = spacy.load('en_core_web_trf')  # Transformer-based model\n",
        "        self.sentiment_model = transformers.pipeline(\n",
        "            'sentiment-analysis',\n",
        "            model='cardiffnlp/twitter-roberta-base-sentiment'\n",
        "        )\n",
        "\n",
        "        # Elasticsearch Configuration\n",
        "        self.es_client = elasticsearch.Elasticsearch([es_host])\n",
        "\n",
        "    def preprocess_tweet(self, tweet_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Advanced preprocessing and feature extraction\n",
        "\n",
        "        Args:\n",
        "            tweet_text (str): Raw tweet text\n",
        "\n",
        "        Returns:\n",
        "            Dict containing processed tweet metadata\n",
        "        \"\"\"\n",
        "        # SpaCy Named Entity Recognition\n",
        "        doc = self.nlp(tweet_text)\n",
        "\n",
        "        # Extract entities with advanced categorization\n",
        "        entities = [\n",
        "            {\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'type': self._categorize_entity(ent)\n",
        "            } for ent in doc.ents\n",
        "        ]\n",
        "\n",
        "        # Transformer-based sentiment analysis\n",
        "        sentiment_result = self.sentiment_model(tweet_text)[0]\n",
        "\n",
        "        # Azure Cognitive Services additional insights\n",
        "        azure_insights = self._get_azure_insights(tweet_text)\n",
        "\n",
        "        return {\n",
        "            'original_text': tweet_text,\n",
        "            'preprocessed_entities': entities,\n",
        "            'sentiment': {\n",
        "                'label': sentiment_result['label'],\n",
        "                'score': sentiment_result['score']\n",
        "            },\n",
        "            'azure_insights': azure_insights,\n",
        "            'linguistic_features': {\n",
        "                'tokens': [token.text for token in doc],\n",
        "                'pos_tags': [token.pos_ for token in doc]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _categorize_entity(self, entity) -> str:\n",
        "        \"\"\"\n",
        "        Advanced entity categorization\n",
        "\n",
        "        Args:\n",
        "            entity: SpaCy Named Entity\n",
        "\n",
        "        Returns:\n",
        "            Specialized entity category\n",
        "        \"\"\"\n",
        "        diabetes_categories = {\n",
        "            'PRODUCT': ['medication', 'medical_device'],\n",
        "            'ORG': ['medical_institution', 'research_center'],\n",
        "            'PERSON': ['medical_professional', 'patient']\n",
        "        }\n",
        "\n",
        "        return diabetes_categories.get(entity.label_, ['generic'])[0]\n",
        "\n",
        "    def _get_azure_insights(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Leverage Azure Cognitive Services for advanced text insights\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of cognitive insights\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Azure text analytics\n",
        "            response = self.text_analytics_client.analyze_sentiment([text])\n",
        "            insights = next(response)\n",
        "\n",
        "            return {\n",
        "                'sentiment_score': insights.confidence_scores,\n",
        "                'sentiment': insights.sentiment\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Azure insights extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def index_tweet(self, processed_tweet: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Index processed tweet to Elasticsearch\n",
        "\n",
        "        Args:\n",
        "            processed_tweet (Dict): Processed tweet metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.es_client.index(\n",
        "                index='diabetes_tweets',\n",
        "                body=processed_tweet\n",
        "            )\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Elasticsearch indexing failed: {e}\")\n",
        "\n",
        "    def advanced_query(self,\n",
        "                       sentiment: Optional[str] = None,\n",
        "                       entity_type: Optional[str] = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Advanced search with multi-dimensional filtering\n",
        "\n",
        "        Args:\n",
        "            sentiment (str, optional): Sentiment filter\n",
        "            entity_type (str, optional): Entity type filter\n",
        "\n",
        "        Returns:\n",
        "            List of matching tweets\n",
        "        \"\"\"\n",
        "        query = {\n",
        "            \"query\": {\n",
        "                \"bool\": {\n",
        "                    \"must\": []\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if sentiment:\n",
        "            query['query']['bool']['must'].append({\n",
        "                \"match\": {\"sentiment.label\": sentiment}\n",
        "            })\n",
        "\n",
        "        if entity_type:\n",
        "            query['query']['bool']['must'].append({\n",
        "                \"nested\": {\n",
        "                    \"path\": \"preprocessed_entities\",\n",
        "                    \"query\": {\n",
        "                        \"match\": {\"preprocessed_entities.type\": entity_type}\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "\n",
        "        results = self.es_client.search(index='diabetes_tweets', body=query)\n",
        "        return results['hits']['hits']\n",
        "\n",
        "def main():\n",
        "    # Example usage with environment variables\n",
        "    analyzer = DiaTweetAnalyzer(\n",
        "        twitter_api_key=os.getenv('TWITTER_API_KEY', ''),\n",
        "        azure_endpoint=os.getenv('AZURE_ENDPOINT', ''),\n",
        "        azure_key=os.getenv('AZURE_KEY', '')\n",
        "    )\n",
        "\n",
        "    # Sample tweets\n",
        "    sample_tweets = [\n",
        "        \"Just got my new insulin pump! #diabetes #tech\",\n",
        "        \"Struggling with managing blood sugar levels today ðŸ˜”\"\n",
        "    ]\n",
        "\n",
        "    for tweet in sample_tweets:\n",
        "        processed_tweet = analyzer.preprocess_tweet(tweet)\n",
        "        analyzer.index_tweet(processed_tweet)\n",
        "        print(f\"Processed Tweet: {processed_tweet}\")\n",
        "\n",
        "    # Advanced query example\n",
        "    results = analyzer.advanced_query(\n",
        "        sentiment='POSITIVE',\n",
        "        entity_type='medical_device'\n",
        "    )\n",
        "    print(\"Query Results:\", results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}